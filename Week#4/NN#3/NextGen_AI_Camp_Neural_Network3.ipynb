{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VhXeY8haioV7"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/NextGen-AI-Camp/curriculum/blob/main/Week%234/NN%233/NextGen_AI_Camp_Neural_Network3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IdOqC60BkUl9"
   },
   "source": [
    "# Deep Learning with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XNrX7_jFYEfa"
   },
   "source": [
    "## Appriximate Sine Function with PyTorch\n",
    "\n",
    "เราจะเริ่ม Neural Network ตอนที่ 3 ด้วยการนำเอาโจทย์เดิม คือ การประมาณค่า sine (Sine Approximation) มาเขียนใหม่ด้วย PyTorch เพื่อทบทวนกระบวนการสร้าง Model และเพื่อให้คุ้นเคยกับคำสั่งใน Library Pytorch\n",
    "\n",
    "จะเริ่มจาก ส่วนของ import library ตามโค้ดด้านล่าง จะเห็นว่าเราคงยังใช้ Numpy อยู่\n",
    "- คำสั่ง import torch เพื่อใช้คำสั่งในการประมวลผล matrix\n",
    "- คำสั่ง import torch.nn as nn เพื่อเรียกใช้ส่วนของ Neural Network\n",
    "- คำสั่ง import torch.optim as optim เพื่อเรียกใช้ส่วนของ optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "08vqL2-KYzZq"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4htWg9KoY8eM"
   },
   "source": [
    "จากนั้นก็เป็นฟังก์ชันในการสร้างข้อมูล sine เหมือนเดิม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d8CZm4X4ZnjE"
   },
   "outputs": [],
   "source": [
    "def generate_points(start=0, end=10, step=0.1):\n",
    "    x_values = np.arange(start, end + step, step)\n",
    "    num_points = len(x_values)\n",
    "    y_values = np.sin(x_values)\n",
    "    noise = np.random.randn(num_points) * 0.1\n",
    "    noisy_y_values = y_values + noise\n",
    "    points = list(zip(x_values, noisy_y_values))\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tT9sv5zzZpgW"
   },
   "source": [
    "สร้างข้อมูลด้วย numpy และแปลงเป็น tensor ของ torch และ ตรวจสอบว่าหากมี GPU (cuda) ก็ให้นำข้อมูลขึ้นไปรันใน GPU  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9IcoOsxZ9Mo"
   },
   "outputs": [],
   "source": [
    "# Check for CUDA\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Generate the dataset\n",
    "points = generate_points()\n",
    "\n",
    "# Prepare the data\n",
    "x = np.array([point[0] for point in points]).reshape(-1, 1)\n",
    "y = np.array([point[1] for point in points]).reshape(-1, 1)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "x = torch.tensor(x, dtype=torch.float32).to(device)\n",
    "y = torch.tensor(y, dtype=torch.float32).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bVh5_6xZ0Ns"
   },
   "source": [
    "กำหนดโครงสร้างของ Model ดังนี้\n",
    "- ชั้นที่ 1 เป็น Hidden Layer ชั้นที่ 1 ประกอบด้วย Input = 1 และ Output = 16 เพื่อนำไปเข้า Hidden Layer ในชั้นถัดไป\n",
    "- ชั้นที่ 2 เป็น Activation Function ของชั้นที่ 1 โดยเลือกใช้ TanH\n",
    "- ชั้นที่ 3 เป็น Hidden Layer ชั้นที่ 2 ประกอบด้วย Input = 16 และ Output = 20 เพื่อนำไปเข้า Output Layer\n",
    "- ชั้นที่ 4 เป็น Activation Function ของชั้นที่ 2 โดยเลือกใช้ TanH เช่นกัน\n",
    "- ชั้นที่ 5 เป็น Output Layer ประกอบด้วย Input 20 และ Output 1\n",
    "\n",
    "สังเกตว่ามิติ (Dimension) ของชั้นที่อยู่ติดกันจะต้องมิติที่เข้ากันได้\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqZJuDWM0l_Z"
   },
   "outputs": [],
   "source": [
    "# Define model using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(1, 16),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(16, 20),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx0R3ySW0ng6"
   },
   "source": [
    "กำหนด loss function และอัลกอรึทึมในการทำ Gradient Descent ในที่นี้จะใช้ Mean Squere Error เป็น Loss Function และ Stochastic Gradient Descent ในการหาจุดต่ำสุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6_IK4LD1bcf"
   },
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20479,
     "status": "ok",
     "timestamp": 1720268588315,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "h0mODMtMXeIM",
    "outputId": "6af9a78f-744f-4f9f-f505-301068e8f046"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "num_epochs = 20000\n",
    "loss_history = []\n",
    "\n",
    "model.train() # train mode      # It's the training time\n",
    "for epoch in range(num_epochs): # for epoch in range\n",
    "\n",
    "    outputs = model(x)          # do the forward pass\n",
    "    loss = criterion(outputs, y)# calculate the loss\n",
    "    optimizer.zero_grad()       # optimize a zero grad\n",
    "    loss.backward()             # loss backward (do backward pass)\n",
    "    optimizer.step()            # optimize step (adjust model parameter)\n",
    "\n",
    "    loss_history.append(loss.item())\n",
    "\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Loss after iteration {epoch}: {loss.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUShdPRj1drv"
   },
   "source": [
    "ส่วนของการ Train จะทำทั้งหมด 20,000 รอบ\n",
    "- บรรทัด optimizer.zero_grad() คือการรีเซ็ทค่า gradient ให้เป็น 0 เพื่อจะได้เก็บค่า Loss ใหม่ตั้งแต่ 0 ของแต่ละ epoch\n",
    "- บรรทัด outputs = model(x) เป็นสั่งให้ทำงานใน forward pass\n",
    "- บรรทัด loss = criterion(outputs, y) เป็นการนำค่าผลลัพธ์จาก forward pass มาหา Loss\n",
    "- บรรทัด loss.backward() เป็นการสั่งให้ทำ backward pass ซึ่งจะได้ค่า gradient ของแต่ละ neuron\n",
    "- บรรทัด optimizer.step() คือ การสั่งให้ปรับ weight และ bias ของ training loop นี้\n",
    "\n",
    "จะเห็นได้ว่า ไม่ว่าจะเป็นการเขียนโปรแกรมเป็นภาษา Python หรือ การทำงานผ่าน Library Pytorch ขั้นตอนการทำงานยังคงเหมือนเดิม และเหมือนกับรูปนี้ เพียงแต่ Pytorch ทำให้โปรแกรมสั้นลงและสะดวกขึ้น แต่การเขียนโปรแกรมยังต้องมีความเข้าใจอย่างดี\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://www.oreilly.com/api/v2/epubs/9781492045519/files/assets/dlcf_0108.png\" width=\"600\"/>\n",
    "\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtgYJG6x1pOa"
   },
   "source": [
    "จากนั้นจะเป็นส่วนของการแสดงผล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 499
    },
    "executionInfo": {
     "elapsed": 1172,
     "status": "ok",
     "timestamp": 1720268589481,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "WQ1pNyBc1sZI",
    "outputId": "6a66bf9f-77e5-400d-b550-4ee6e5aa59f1"
   },
   "outputs": [],
   "source": [
    "def plot_output(points, model, loss, device):\n",
    "    x_values, noisy_y_values = zip(*points)\n",
    "    x_values = torch.tensor(x_values, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    noisy_y_values = np.array(noisy_y_values)\n",
    "    y_values = np.sin(x_values.cpu().numpy()).flatten()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted_y_values = model(x_values).cpu().numpy().flatten()\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(x_values.cpu().numpy(), noisy_y_values, label='Noisy Data', color='blue', s=10)\n",
    "    plt.plot(x_values.cpu().numpy(), y_values, label='True Sine', color='red', linestyle='--')\n",
    "    plt.plot(x_values.cpu().numpy(), predicted_y_values, label='Model Prediction', color='green')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Model Predictions vs True Sine and Noisy Data')\n",
    "    plt.legend()  # Add this line to ensure legend is created\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss, label='Loss')  # Add label for the loss plot\n",
    "    plt.title('Loss over time')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()  # Add this line to ensure legend is created\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Plot the output\n",
    "plot_output(points, model, loss_history, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3-bYmRq3s6J"
   },
   "source": [
    "จะเห็นได้ว่าผลลัพธ์การทำงานออกมาคล้ายเดิม"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0sBBOz83zLF"
   },
   "source": [
    "## Generative AI และ Discriminative AI\n",
    "\n",
    "ในการใช้งาน AI ปัจจุบัน อาจแยกการใช้งานออกเป็น 2 แบบใหญ่ๆ\n",
    "- แบบแรกเรียกว่า Discriminative AI หรือ ปัญญาประดิษฐ์เชิงจำแนก ซึ่งมุ่งเน้นการจำแนกหรือการทำนายผลลัพธ์จากข้อมูลที่มีอยู่ โดยเน้นการแยกแยะความแตกต่างระหว่างข้อมูลในกลุ่มหรือประเภทต่างๆ หรือตัวแปรเป้าหมายจากข้อมูลอินพุต ซึ่งสิ่งที่เราเรียนไปจะอยู่ในกลุ่มนี้\n",
    "- แบบที่สองเรียกว่า Generative AI หรือ ปัญญาประดิษฐ์เชิงกำเนิด ซึ่งมุ่งเน้นในการสร้างข้อมูลใหม่ที่มีลักษณะคล้ายคลึงกับข้อมูลต้นแบบ เช่น การสร้างภาพ, ข้อความ, หรือเสียงใหม่จากการเรียนรู้จากชุดข้อมูลที่มีอยู่ ซึ่ง AI แบบหลังนี่เองที่สร้างความตื่นตัวขึ้นมาอย่างมาก เช่น Midjourney, DELL-E, ChatGPT, Gemini หรืออื่นๆ ถือว่าเป็น AI ในกลุ่มนี้\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7bm8HQAFEOW"
   },
   "source": [
    "## ประเภทของปัญญาประดิษฐ์เชิงจำแนก\n",
    "\n",
    "ใน Discriminative AI หรือ ปัญญาประดิษฐ์เชิงจำแนกนั้น ยังสามารถจำแนกประเภทได้เป็น 2 แบบ คือ\n",
    "- Regression คือ AI ที่สร้างขึ้นมาเพื่อทำนายค่าของตัวแปรเป้าหมาย  จากค่าของตัวแปรที่ป้อนให้ ตัวอย่างของ AI ประเภทนี้ เช่น การประมาณค่าในสมการเส้นตรง การประมาณค่าของฟังก์ชัน Sine ที่เราทำมาแล้ว หรืออาจจะเป็นการทำนายราคาบ้านจากขนาดและทำเลที่ตั้ง หรือการทำนายผลการขายจากงบประมาณการโฆษณา หรือ การทำนายราคารถยนต์ หรืออื่นๆ ซึ่งเป้าหมายของ Regression คือการหาความสัมพันธ์และการทำนายค่าที่แม่นยำที่สุดจากข้อมูลที่มีอยู่ โดยข้อมูลที่ทำนายมักจะมีลักษณะเป็นตัวเลข\n",
    "- Classification ซึ่งแปลว่าจำแนก ดังนั้นสิ่งที่ AI กลุ่มนี้ทำ คือ การจำแนกข้อมูลออกเป็นกลุ่มๆ ซึ่งเรามักเรียกว่า Class เช่น จำแนกว่าเป็นหมาหรือเป็นแมว หรือ จำแนกว่าเป็นวัตถุอะไร (เรียกว่า Object Detection) หรือ จำแนกใบหน้าคน (Face Recognition) หรือ จำแนกว่า เมล์นี้เป็น SPAM หรือไม่ หรือ เป็นโรคหรือไม่เป็นโรค หรือ จำแนกอารมณ์จากหน้า หรืออื่นๆ ที่สามารถกำหนดค่าเป้าหมายออกเป็นกลุ่มๆ ได้ ซึ่ง AI กลุ่มนี้ยังแบ่งออกเป็น ประเภทย่อย เช่น\n",
    ">- Binary Classification ก็คือ การแยกออกเป็นแค่ 2 กลุ่ม (class) เช่น เป็นหมาหรือเป็นแมว เป็น SPAM หรือไม่ SPAM เป็นโรคหรือไม่เป็นโรค\n",
    ">- Multiclass Classification คือ การแยกออกมากกว่า 2 กลุ่ม (class) เช่น แยกประเภทพันธ์สุนัข\n",
    ">- Multilabel Classification คือ ผลลัพธ์อาจมีหลายกลุ่ม เช่น การ Tag บทความด้วยประเภท\n",
    ">- ประเภทอื่นๆ ที่ไม่กล่าวในที่นี้ เพราะมีรายละะเอียดมาก และ ยังไม่จำเป็นต้องรู้ เช่น Imbalanced Classification, Ordinal Classification, Hierarchical Classification, ฯลฯ\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*xs6Jr4iAPvoqszF9JgDWOA.png\" width=\"600\"/>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3GYZnekJb_D"
   },
   "source": [
    "### ความแตกต่างของ Classification จาก Regression\n",
    "\n",
    "ในการเรียนที่ผ่านมา เรากล่าวถึงเฉพาะการทำงานในแบบ Regression ไม่ว่าจะเป็นการประมาณค่าสมการเส้นตรง หรือ การประมาณสมการค่า Sine แต่สำหรับวิธีการของ Classification จะมีความแตกต่างออกไปเล็กน้อย การทำงานของ Input Layer และ Hidden Layer จะเหมือนเดิม แต่สิ่งที่แตกต่างออกไป คือ Output Layer โดยแทนที่ จะมีการทำงานดังรูป (กรณี regrssion) ซึ่งจะเห็นได้ว่า ใน Output Layer จะมีเพียง Neuron เดียว เพราะต้องการข้อมูลออกมาเป็นตัวเลขเท่านั้น\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*Gh5PS4R_A5drl5ebd_gNrg@2x.png\" width=\"400\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "แต่สำหรับการทำงานแบบ Classification จะใช้ Output Layer หลาย Neuron\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/30541010.png\" width=\"400\"/>\n",
    "</div>\n",
    "<br>\n",
    "\n",
    "อย่างไรก็ตามยังมีกรณีพิเศษ คือ กรณีของ Binary Classification เราสามารถใช้ Output Layer แบบ Neuron เดียวก็ได้ เนื่องจาก หากกำหนดให้ Output เป็น 0 หรือ 1 ก็สามารถใช้โมเดลแบบที่มี Output Layer แบบ Neuron เดียวมาทำเป็น Binary Classification ก็ได้ หรือจะใช้แบบมี Output Layer 2 Neuron ก็ได้\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://www.researchgate.net/publication/330120030/figure/fig1/AS:735637925797888@1552401157053/Deep-Neural-Network-architecture.ppm\" width=\"400\"/>\n",
    "</div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0jkFN9CTWVs"
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "ในกรณีที่ต้องการคำตอบแบบ 2 ค่า เช่น “เกิด” กับ “ไม่เกิด” หรือ “เสี่ยง” กับ “ไม่เสี่ยง” หรือ “เป็น” กับ “ไม่เป็น” หรือที่เรียกกันว่า Binary Classification เราจะใช้วิธีการที่เรียกว่า Logistic Regression โดยจะใช้วิธีการคำนวณแบบตัวเลขเหมือนเดิม แต่จะมีกรรมวิธีที่จะตีความตัวเลขที่ได้ ว่าอยู่ในกลุ่มไหน เช่น สมมติมีข้อมูลต่อไปนี้\n",
    "\n",
    "| ลำดับที่  | ตัวเลขวิเคราะห์การเป็น Spam | เป็น Spam หรือไม่ | ความน่าจะเป็น (Logistic Regression)  |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "|1\t| 1.8 |\tY\t| 0.8581 |\n",
    "|2\t| -0.8 |\tN\t| 0.3100 |\n",
    "|3\t| 2.2\t| Y\t| 0.9002 |\n",
    "|4\t| -1.7\t | N\t| 0.1545 |\n",
    "|5\t| 0.4\t| Y\t| 0.5987 |\n",
    "\n",
    "- ตัวเลขในคอลัมน์ที่ 2 เป็นผลลัพธ์ (Raw Output) ที่ได้จาก Model\n",
    "- ข้อมูลในคอลัมน์ที่ 3 เป็น Label หรือคำตอบจริงว่าเป็น Spam หรือไม่เป็น\n",
    "\n",
    "หน้าที่อย่างแรก คือ จะต้องแปลงข้อมูลที่เป็นตัวเลขในคอลัมน์ที่ 2 ให้เป็น ความน่าจะเป็นคือให้ได้ตามคอลัมน์ที่ 4 ซึ่งความน่าจะเป็นจะมีค่าอยู่ระหว่าง 0-1\n",
    "\n",
    "ถ้ายังพอจำกันได้ ในเรื่องของ Activation Function มีฟังก์ชันหนึ่งที่ให้ Output อยู่ระหว่าง 0-1 ใช่แล้วครับ เรากำลังพูดถึง Sigmoid นั่นเอง ดังนั้นแปลว่า ถ้าเราจะนำเอาข้อมูลในคอลัมน์ที่ 2 เช่น 1.8 มาคำนวณด้วย Sigmoid ก็จะได้เป็นความน่าจะเป็น 0.85\n",
    "\n",
    "จากนั้นก็ใช้ค่าคงที่เพื่อบอกว่าเป็น หรือ ไม่เป็นต่อไป เช่น ถ้า > 0.5 ก็เป็น Spam แต่ < 0.5 ก็ไม่เป็น Spam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFqDznIhi706"
   },
   "source": [
    "### สร้าง Model Neural Network สำหรับ Logistic Classification\n",
    "\n",
    "เอาละ! เราจะมาลองใช้ Sigmoid เพื่อทำ Binary Classification กันดู แต่ก่อนอื่น เราจะมาสร้างโมเดล Neural Network กัน โดยการสร้าง Model เราจะไม่ใช้ `nn.Sequential` ตามแบบก่อนหน้านี้ แต่จะใช้วิธีการสร้าง Subclass ของ `nn.Model` ซึ่งแสดงตามโปรแกรมด้านล่าง\n",
    "\n",
    "ความแตกต่างของการสร้าง Model ในแบบ `nn.Sequential` กับแบบที่สร้าง Subclass ของ `nn.Model` คือ การสร้าง Model โดยใช้ `nn.Sequential` นั้นจะสั้น เหมาะกับโมเดลที่ไม่ซับซ้อน แต่ Model ที่สร้างจาก Subclass ของ `nn.Model` จะยาวกว่า แต่สามารถใส่รายละเอียดปลีกย่อยได้ ทำให้เหมาะสมกับโมเดลที่มีความซับซ้อน\n",
    "\n",
    "เนื่องจากต่อไปเราจะใช้โมเดลที่มีความซับซ้อนมากขึ้นเรื่อยๆ ดังนั้นต่อไปเราจะใช้แบบที่สร้าง Subclass ของ `nn.Model` เป็นหลัก"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgPR6TNetkj0"
   },
   "outputs": [],
   "source": [
    "# Create a Logistic Regression model class\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LogisticRegressionModel(nn.Module): # <- almost everything in PyTorch is a nn.Module\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Create 2 nn.Linear layers capable of handling X and y input and output shapes\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BocxxM2w1x5k"
   },
   "source": [
    "- ในบรรทัด `self.layer_1 = nn.Linear(in_features=2, out_features=5)` จะเป็นการสร้าง Layer ของ Neuron โดยกำหนดให้ Layer นี้มี 2 Input และมี 5 Output ซึ่งหมายความว่า Layer นี้จะมี Neuron จำนวน 5 ตัว\n",
    "- ในบรรทัด `self.layer_2 = nn.Linear(in_features=5, out_features=1)` จะเป็นการสร้าง Layer ของ Neuron โดยกำหนดให้ Layer นี้มี 5 Input และมี 1 Output ซึ่งหมายความว่า Layer นี้จะมี Neuron จำนวน 1 ตัว\n",
    "\n",
    "ซึ่งหากใช้ [tensorflow playground](https://playground.tensorflow.org) วาด ก็จะได้ Neuron Network ประมาณนี้\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://github.com/khthana/AI_resource/blob/main/TF1.JPG?raw=true\" width=\"900\"/><div>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3anLhyfF-DAW"
   },
   "source": [
    "- สำหรับใน `forward` ซึ่งมี `return self.layer_2(self.layer_1(x))` ก็คือการนำผลลัพธ์ของ Layer 1 ส่งต่อให้ Layer 2 และส่งผลกลับมาเท่านั้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdXW3sZjXXal"
   },
   "source": [
    "### การสุ่มและความสำคัญของ Seed\n",
    "\n",
    "ก่อนจะไปทดสอบการทำงานของ Model ขออธิบายเรื่องการสุ่มในระบบคอมพิวเตอร์ซักเล็กน้อย\n",
    "\n",
    "การสุ่ม (Random) ในคอมพิวเตอร์จะต่างจากการสุ่มในโลกความเป็นจริง เพราะการสุ่มในโลกความเป็นจริง เช่น โยนหัวก้อย ผลที่เกิดขึ้นจะคาดเดาไม่ได้เลย แต่การสุ่มในคอมพิวเตอร์นั้นไม่ใช่การสุ่มจริง โดยจะมีชื่อเรียกเต็มๆ ว่า pseudo random หรือแปลว่าการสุ่มเทียม เพราะคอมพิวเตอร์ไม่สามารถโยนหัวก้อยได้ จึงต้องใช้ Algorithm ในการสร้างเลขสุ่มขึ้นมา ลองดูโปรแกรมต่อไปนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1720268589482,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "wuyiFAxPtN3o",
    "outputId": "f1684639-538b-41cd-da99-8cb4ff0af3c7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# กรณีไม่มี manual_seed\n",
    "print(\"ไม่มี manual_seed 1:\")\n",
    "for i in range(3):\n",
    "    print(torch.rand(3))\n",
    "print(\"ไม่มี manual_seed 2:\")\n",
    "for i in range(3):\n",
    "    print(torch.rand(3))\n",
    "print(\"มี manual_seed 1:\")\n",
    "# กำหนด manual_seed = 12\n",
    "torch.manual_seed(12)\n",
    "for i in range(3):\n",
    "    print(torch.rand(3))\n",
    "print(\"มี manual_seed 2:\")\n",
    "torch.manual_seed(12)\n",
    "for i in range(3):\n",
    "    print(torch.rand(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-53y-MLaVa8"
   },
   "source": [
    "จะเห็นได้ว่ากรณีที่ไม่ได้ใช้ manual seed (บางทีเรียก random seed) ค่าสุ่มที่ได้จะเปลี่ยนไปเรื่อยๆ แต่หากใช้ manual seed แล้วค่าที่ได้จะเป็นค่าชุดเดิม\n",
    "\n",
    "**ถามว่า สำคัญอย่างไร**\n",
    "\n",
    "เนื่องจากค่า parameter ของ Model ล้วนแต่เกิดจากการ random ทั้งสิ้น สมมติว่าเราทำงานไปรอบที่ 1 กำหนดค่า hyperparameter ต่างๆ (learning rate, epoch, จำนวน node ในแต่ละ layer แล้วพบว่าได้ผลดี แต่พอมารันในรอบที่ 2 แล้วผลเปลี่ยนไปจากเดิม เราก็คงไม่พอใจใช่มั้ยครับ ดังนั้นเพื่อให้ผลการรันยังเหมือนเดิมทุกครั้ง การกำหนด manual_seed เอาไว้ก็จะทำให้ผลการทำงานเหมือนเดิมเสมอ ทำให้ในการปรับค่า hyperparameter สามารถทำได้อย่างเป็นระบบ เหมือนกับการควบคุมสภาพแวดล้อมของการทดลอง\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uuY7kmWl_zM"
   },
   "source": [
    "### ทดสอบการทำงานของ Logistic Regression\n",
    "\n",
    "หลังจากนอกเรื่องไปพูดถึงเรื่อง manual seed คราวนี้เรากลับมาเข้าเรื่องเดิม คือ หลังจากที่กำหนด Model แล้วก็มาดูข้อมูลภายใน Model กันดูบ้าง\n",
    "\n",
    "โปรแกรมด้านล่างนี้ เริ่มจากการกำหนด manual_seed ขึ้นมาค่าหนึ่ง\n",
    "จากนั้นกำหนด Model\n",
    "และนำ Model parameter เข้าไปที่ list และสั่งพิมพ์ parameter ออกมาดู จะเห็นว่ามี parameter ดังนี้\n",
    "1.  ค่า weight จำนวน 10 ตัว ซึ่งเก็บอยู่ที่ hidden layer จำนวน 5 node โดยมี node ละ 2 ตัว (ตาม input)\n",
    "2. ค่า bias ของ neuron ของ hidden layer จำนวน 5 ตัว\n",
    "3. ค่า weight จำนวน 5 ตัว ซึ่งเก็บอยู่ที่ output layer\n",
    "4. ค่า bias ของ neuron ของ output layer\n",
    "\n",
    "และ ถ้าสังเกตให้ดีจะมีคำว่า requires_grad=True ซึ่งแปลว่า ข้อมูลที่เก็บใน tensor นี้ จะเก็บค่า gradient เอาไว้ด้วยเพื่อใช้ในการคำนวณ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1720268589482,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "6m-GPMOIsEZl",
    "outputId": "5f6d57fe-1c62-4e73-9a87-c73391e6a2ae"
   },
   "outputs": [],
   "source": [
    "# Set manual seed since nn.Parameter are randomly initialzied\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
    "model_0 = LogisticRegressionModel()\n",
    "\n",
    "# Check the nn.Parameter(s) within the nn.Module subclass we created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEKiitq8RUHh"
   },
   "source": [
    "การแสดงข้อมูลสามารถทำได้อีกวิธี คือ ใช้ `.state_dict()` ซึ่งจะมีชื่อของข้อมูลแสดงให้เห็นด้วย\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1720268589482,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "FW79c0vORpah",
    "outputId": "7e9221a5-d4bc-485d-8335-d0aecb2ff21d"
   },
   "outputs": [],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpshjxOUnczO"
   },
   "source": [
    "คราวนี้เราจะสมมติข้อมูล เพื่อทดลองนำไปเข้า Model โดยสมมติว่ามี 5 ข้อมูลตามโปรแกรม ซึ่งตามปกติข้อมูล Input ที่เป็น Classification นอกจากจะมีข้อมูลดิบ แล้วยังต้องมีผลเฉลย หรือที่มักเรียกกันว่า Label\n",
    "\n",
    "จากนั้นนำ data แปลงเป็น Tensor แล้วเก็บในตัวแปร X จากนั้น feed เข้าสู่ Model และเก็บไว้ที่ตัวแปร untrain_preds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1720268589482,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "RKcVmABesZh8",
    "outputId": "c9f538c8-fc95-47e4-c019-e10a24382381"
   },
   "outputs": [],
   "source": [
    "data= [[0.7542, 0.2315],\n",
    "        [-0.7562, 0.1533],\n",
    "        [-0.8154, 0.1733],\n",
    "        [-0.3937, 0.6929],\n",
    "        [0.4422, -0.8967]]\n",
    "X = torch.tensor(data)  # Input features\n",
    "\n",
    "label = [1., 1., 1., 1., 0.]\n",
    "y = torch.tensor(label)  # Target labels\n",
    "\n",
    "untrained_preds = model_0(X)  # Feed data to Model and output prediction\n",
    "\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\n",
    "# Assuming y_test has the same length and format as y\n",
    "print(f\"Length of test samples: {len(y)}, Shape: {y.shape}\")  # Use y instead of y_test\n",
    "print(f\"\\nPredictions:\\n{untrained_preds}\")\n",
    "print(f\"\\nTest labels: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ga05I0qpcgw"
   },
   "source": [
    "จะเห็นว่าข้อมูลที่ทำนาย (Prediction) ออกมาจะเป็นตัวเลขที่มีขนาดไม่จำกัด\n",
    "แต่ Label หรือผลทำนาย เป็นตัวเลข 1 หรือ 0 ดังนั้น จะต้องทำ 2 ขั้นตอน คือ\n",
    "1. ต้องแปลง ตัวเลข ให้อยู่ในรูปแบบของความน่าจะเป็นระหว่าง 0-1 ก่อน\n",
    "2. สร้างจุดตัด เช่น 0.5 เพื่อเปลี่ยนเป็น 0 กับ 1\n",
    "\n",
    "การเปลี่ยนตัวเลขใดๆ ให้เป็นความน่าจะเป็น 0-1 โดยใช้ sigmoid\n",
    "\n",
    ">$\n",
    "\\text{Sigmoid} = \\log\\left(\\frac{p}{1 - p}\\right)\n",
    "$\n",
    "\n",
    "ดังนั้นเราก็เพียงเอาตัวเลขไปผ่าน Sigmoid ก็จะได้ความน่าจะเป็นออกมา จากนั้นนำมาผ่าน round อีกชั้น ก็จะได้เป็น 1 หรือ 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1720268589482,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "4AT4fNHnvVyr",
    "outputId": "3d01af3e-d135-46ae-b1eb-ddeb88fe657c"
   },
   "outputs": [],
   "source": [
    "# View the outputs of the forward pass on the test data\n",
    "y_logits = model_0(X)\n",
    "print(f\"Logit : \\n{y_logits}\\n\")\n",
    "\n",
    "# Use sigmoid on model logits\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "print(f\"Probability of prediction : \\n{y_pred_probs}\\n\")\n",
    "\n",
    "print(f\"Prediction : \\n{torch.round(y_pred_probs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WSe0bc9t3HJ"
   },
   "source": [
    "### Multiclass Classification\n",
    "\n",
    "คือกระบวนการจำแนกประเภทของข้อมูลออกเป็นหลายคลาส (มากกว่า 2 คลาส) โดยที่แต่ละตัวอย่างข้อมูลจะถูกจัดให้อยู่ในคลาสใดคลาสหนึ่งจากหลายคลาสที่มีอยู่ โดยอาจใช้งานได้หลากหลาย เช่น\n",
    "\n",
    "1. การจำแนกประเภทของดอกไม้: เช่น การจำแนกดอกไม้เป็นสายพันธุ์ต่างๆ เช่น Setosa, Versicolor, Virginica\n",
    "2. การรู้จำภาพ: เช่น การจำแนกภาพเป็นประเภทต่างๆ เช่น แมว, สุนัข, รถยนต์, ต้นไม้\n",
    "3. การวิเคราะห์ข้อความ: เช่น การจำแนกประเภทของอีเมลเป็นหมวดหมู่ต่างๆ เช่น งาน, ส่วนตัว, สแปม\n",
    "\n",
    "การทำงานใน Multiclass Classification จะมีขั้นตอนเพิ่มมากกว่า Binary Classification หลายอย่าง ได้แก่\n",
    "\n",
    "1. การเตรียมข้อมูล Input หรือ การแปลงข้อมูล Input เนื่องจากในกระบวนการทำงานทุกอย่างจะต้องเป็นตัวเลขเสมอ ดังนั้น สมมติว่าจะสร้าง AI ที่แยะแยะระหว่าง กระเพรา โหระพา และ แมงลัก ก็จะต้องมีการทำ Label Encoding หรือ แปลงคลาสต่างๆ ให้เป็นค่าตัวเลข เช่น กระเพรา = 0, โหระพา = 1, แมงลัก = 2 นอกจากนั้นยังจะต้องแปลงข้อมูลให้อยู่ในรูปของ One-Hot Encoding (เวกเตอร์ที่มี 1 เพียงที่เดียว) โดย แปลงคลาสต่างๆ ให้เป็นเวกเตอร์ เช่น [1, 0, 0] สำหรับ กระเพรา, [0, 1, 0] สำหรับ โหระพา, [0, 0, 1] สำหรับ แมงลัก\n",
    "\n",
    "ซึ่งในตอนที่ยกตัวอย่างการทำงาน ก็จะเห็นวิธีการได้ชัดเจน\n",
    "\n",
    "2. ส่วนของ Output Layer จะต้องเปลี่ยนการคำนวณ Logistic Regression แบบหลายคลาส (Multinomial Logistic Regression) จากเดิมที่ใช้ Sigmoid ไปใช้ฟังก์ชันแบบใหม่ที่เรียกว่า Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30gPq1dgx-_y"
   },
   "source": [
    "### Softmax\n",
    "\n",
    "Softmax อาจถือได้ว่าเป็น Actication Function ตัวหนึ่ง โดยมีบทบาทคล้ายกับ Sigmoid แต่ในขณะที่ Sigmoid (ที่ผ่าน round อีกรอบ) มีผลทำให้เกิดตัวเลขเพียง 0 และ 1 เพื่อใช้ในการทำนาย Binary Classification แต่ Softmax จะให้ผลลัพธ์เป็นตัวเลขความน่าจะเป็นที่จะตรงกับข้อมูลแต่ละกลุ่ม การทำงานของ Softmax แสดงดังรูป\n",
    "\n",
    "<div>\n",
    "\n",
    "<img src=\"https://images.contentstack.io/v3/assets/bltac01ee6daa3a1e14/blte5e1674e3883fab3/65ef8ba4039fdd4df8335b7c/img_blog_image1_inline_(2).png?width=2240&disable=upscale&auto=webp\" width=\"400\"/><div>\n",
    "<br>\n",
    "\n",
    "จะเห็นว่าหลักของการทำงาน คือ ต้องแปลงค่าใน Output Layer ซึ่งออกมาเป็นตัวเลขให้เป็นความน่าจะเป็นที่ Input จะอยู่ในกลุ่มต่างๆ\n",
    "- อาจจะใช้วิธีนำตัวเลขของแต่ละตัวมาบวกกัน แล้วใช้เป็นตัวหารของตัวเลขแต่ละตัวก็ได้ แต่จะมีปัญหาข้อมูลบวกลบที่อาจจะหักล้างกันเอง\n",
    "- จึงใช้ยกกำลังเพื่อป้องกันค่าติดลบ คราวนี้พอพิจารณาเรื่องยกกำลัง เขาก็เลือก $e^x$ โดยมีเหตุผล คือ ยกกำลังเพราะหาอนุพันธ์ง่ายและค่าไม่ติดลบ\n",
    "- ดังนั้นก็เริ่มจากนำทุกค่ามาทำ $e^x$ เช่น $e^{z1}+e^{z2}+e^{z3}+e^{z4}+e^{z5}$\n",
    "- จากนั้นนำมารวมกัน (Sum) และ นำมาเป็นตัวหาร\n",
    "- ตย. $e^{1.3} / (e^{1.3}+e^{5.1}+e^{2.2}+e^{0.7}+e^{1.1})$ จะได้เป็น 0.02\n",
    "\n",
    "วิธีการแบบนี้เรียกว่า Softmax ซึ่งสังเกตุว่า ผลลัพธ์ของทุกตัวรวมกันจะเท่ากับ 1.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq6ziEkbomnB"
   },
   "source": [
    "### Cross Entropy Loss\n",
    "\n",
    "สรุปว่า Activation Function ของ Output Layer กำหนดให้เป็น Softmax เพื่อแสดงความน่าจะเป็นที่ Input นั้นจะเป็น output ตัวไหนมากที่สุด\n",
    "\n",
    "คราวนี้จะพิจารณาหาค่า Loss จาก Softmax ซึ่งจะใช้หลักการที่เรียกว่า Cross Entropy Loss\n",
    "\n",
    "- Entropy เป็นการวัดความไม่แน่นอน (uncertainty) หรือความสุ่ม (randomness) ของการแจกแจงความน่าจะเป็น ดังนั้น Cross Entropy Loss จึงแปลว่าการวัดค่า Loss ของความน่าจะเป็นเมื่อเทียบกับความน่าจะเป็นทั้งหมด\n",
    "\n",
    ">$ \\text{Cross Entropy Loss} = - \\sum_{i} y_i \\log(p_i) $\n",
    "\n",
    "โดยที่:\n",
    "- $ y_i $ คือค่าจริงของตัวอย่างที่ $i$ (one-hot encoded)\n",
    "- $ p_i $ คือค่าความน่าจะเป็นที่โมเดลคาดการณ์สำหรับคลาสที่ $i$\n",
    "\n",
    "ลองมาดูการใช้งานสมมติว่า\n",
    "- ข้อมูล $y_i$ คือ $[0,1,0,0,0]$ (one-hot encoded)\n",
    "- ความน่าจะเป็น $p_i$ คือ $[0.02, 0.90, 0.05, 0.01, 0.02]$\n",
    "\n",
    ">$Cross Entropy Loss = $\n",
    "$−[0⋅log(0.02)+1⋅log(0.90)+0⋅log(0.05)+0⋅log(0.01)+0⋅log(0.02)] $\n",
    "\n",
    ">$= −log(0.90) $\n",
    "\n",
    ">$= 0.105360516 $\n",
    "\n",
    "นี่เป็นกรณีที่ผลลัพธ์กับเป้าหมายตรงกัน\n",
    "\n",
    "แต่หากเป็นกรณีที่ผลลัพธ์กับเป้าหมายไม่ตรงกัน เช่น สมมติว่า $y_i$ คือ $[1,0,0,0,0]$\n",
    "\n",
    ">$Cross Entropy Loss= $\n",
    "\n",
    ">$−[1⋅log(0.02)+0⋅log(0.90)+0⋅log(0.05)+0⋅log(0.01)+0⋅log(0.02)] $\n",
    "\n",
    ">$=−log(0.02) $\n",
    "\n",
    ">$= 3.912023005428146$\n",
    "\n",
    "ซึ่งจะเห็นได้ชัดเจนว่ากรณีผลลัพธ์ตรงกับเป้าหมายจะให้ค่า Loss ที่ต่ำ และหากผลลัพธ์ไม่ตรงกับเป้าหมาย จะให้ค่า Loss ที่สูง\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_0SdXmuP0DUB"
   },
   "source": [
    "# การ Classification ข้อมูลตัวเลขไทย\n",
    "\n",
    "หลังจากที่ได้อธิบาย Loss Function ที่ใช้กับ Multiple Classification กันไปแล้ว ต่อไปเราจะมาทำงานกับข้อมูลจริง โดยข้อมูลนี้เป็นข้อมูลจาก [https://github.com/kittinan/thai-handwriting-number](https://github.com/kittinan/thai-handwriting-number) แต่นำมา clean ข้อมูลบางส่วนอีกที โดยมีตัวอย่างข้อมูลดังนี้\n",
    "\n",
    "<div>\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/kittinan/thai-handwriting-number/master/docs/img/thai-handwriting-number.png\" width=\"800\"/><div>\n",
    "<br>\n",
    "\n",
    "เราจะนำข้อมูลมาทำการ Classification ว่าเป็นเลขใดใน 0-9 ดังนั้นโจทย์นี้จะมี คลาสคำตอบอยู่ทั้งหมด 10 คลาส\n",
    "\n",
    "ในการนำข้อมูลเข้ามา เราจะใช้คำสั่ง git clone ตามโปรแกรมด้านล่าง เพื่อ copy ข้อมูลจาก repository มายัง google colab ของเรา\n",
    "\n",
    "**ข้อควรระวัง**\n",
    "ข้อมูลใน google colab จะอยู่เฉพาะตอนที่รัน เมื่อออกจากการทำงานข้อมูลจะถูกลบทัังหมด ดังนั้นต้องทำใหม่ทุกครั้ง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1995,
     "status": "ok",
     "timestamp": 1720842329419,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "zgFOhG5SKHo9",
    "outputId": "c66da4aa-0730-43ca-f60e-27c2d27fa2c3"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/NextGen-AI-Camp/curriculum/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DHQVEj2GF_S"
   },
   "source": [
    "เนื่องจากข้อมูลจะเป็นแบบ compress ดังนั้นจะต้องแตกไฟล์โดยใช้คำสั่งข้างล่างนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1720842481547,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "wmd-XnVBE0mY",
    "outputId": "c1a9007e-4d8d-4c91-d172-9fe296e2a356"
   },
   "outputs": [],
   "source": [
    "!unzip /content/curriculum/Dataset/thai-handwritten-dataset.zip -d /content/curriculum/Dataset/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ciF6OPBPKsss"
   },
   "source": [
    "หลังจากที่นำไฟล์เข้ามาแล้ว เมื่อคลิกที่เครื่องหมาย Files ที่ด้านซ้ายของ google colab จะพบว่ามีไฟล์ปรากฏขึ้นมา และ หากคลิกเข้าไปในกล่องเลข 1 จะพบว่ามีไฟล์จำนวนมาก\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://github.com/khthana/AI_resource/blob/main/number_folder.JPG?raw=true\" width=\"300\"/><div>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQDWiicM_QuT"
   },
   "source": [
    "## การโหลดข้อมูล\n",
    "\n",
    "### Library สำหรับจัดการไฟล์\n",
    "\n",
    "ก่อนอื่นต้องขออธิบายการใช้งาน Library สำหรับจัดการไฟล์ก่อน เพราะใช้อยู่หลายตัวเหมือนกัน\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejlbEY4ROOs8"
   },
   "source": [
    "#### Library os\n",
    "\n",
    "os เป็น Library ครอบจักรวาล คือ ใช้สำหรับให้ python สามารถใช้บริการของระบบปฏิบัติการได้ สำหรับโปรแกรมของเราจะใช้เฉพาะ Module ย่อยของ os คือใช้เฉพาะที่เกี่ยวกับ path จะใช้ในการสร้าง path ก็คือเอา string ที่เป็นลำดับของ folder มาต่อกันแล้วใส่เครื่องหมาย \"/\" คั่น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1720842712261,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "vD7A4XjdOG9L",
    "outputId": "2bc57b2b-a743-4499-905c-cb75d5d73f77"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"curriculum/Dataset\"\n",
    "path = os.path.join(directory, \"thai-handwritten-dataset\", \"*\", \"*\")\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgGWuCWnARB0"
   },
   "source": [
    "#### Library \"glob\"\n",
    "\n",
    "glob เป็น library สำหรับค้นหาชื่อไฟล์ โดยมีความสามารถในการค้นหาชื่อไฟล์ที่เก่งมาก เนื่องจากสามารถใช้ เครื่องหมาย * (หมายถึงทุกตัวอักษรทุกความยาว) และ ? หมายถึง ทุกตัวอักษรยาว 1 ตัว ในการค้นหาชื่อไฟล์ได้\n",
    "\n",
    "จากโปรแกรมข้างล่างนี้จะเห็นว่า glob จะทำหน้าที่ค้นหาไฟล์ที่อยู่ใน folder thai-handwritten-dataset ในทุกไฟล์หรือโฟลเดอร์ และค้นหาไฟล์หรือโฟลเดอร์ในชั้นย่อยลงไปอีกชั้น โดยข้อมูลที่ค้นหาได้จะเก็บใน List ซึ่งจะเห็นว่ามีไฟล์ทั้งหมด 1750 ไฟล์\n",
    "\n",
    "```\n",
    "การทำงานเทียบเท่า dir thai-handwritten-dataset\\*\\* /s\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 570,
     "status": "ok",
     "timestamp": 1720750565585,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "kyryuUtnA2Li",
    "outputId": "e8df51aa-f016-41e2-c38c-acac58af3771"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "directory = \"curriculum/Dataset/\"\n",
    "path = os.path.join(directory, \"thai-handwritten-dataset\", \"*\", \"*\")\n",
    "all_file = glob(path)\n",
    "print(len(all_file))\n",
    "all_file[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ad0LiIxT4Dz"
   },
   "source": [
    "#### Library PIL\n",
    "\n",
    "PIL ย่อมาจาก Python Imaging Library ทำหน้าที่ในการจัดการกับข้อมูลประเภท Image บางทีก็เรียกว่า PILLOW สามารถการปรับแต่งพิกเซล, การกรองภาพ, การปรับปรุงภาพ, เพิ่มข้อความในรูปภาพ, ฟังก์ชันการประมวลผลภาพพื้นฐาน\n",
    "\n",
    "ดังนั้นจึงสามารถนำภาพที่อยู่ในไฟล์มาแสดงได้\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 385,
     "status": "ok",
     "timestamp": 1720843002713,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "9sWQCFhXHuHe",
    "outputId": "1dab57b3-e2d6-4f1a-cefb-9e4fef7677c4"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "directory = \"curriculum/Dataset/\"\n",
    "path = os.path.join(directory, \"thai-handwritten-dataset\", \"*\", \"*\")\n",
    "all_file = glob(path)\n",
    "\n",
    "idx = 100\n",
    "image = Image.open(all_file[idx])\n",
    "print(image.size)\n",
    "image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ff_r21wsksfU"
   },
   "source": [
    "## การแบ่งข้อมูลใน Dataset เป็น Training Set และ Test Set\n",
    "\n",
    "เมื่อเรามีข้อมูลเรียบร้อยแล้ว ก่อนที่เราจะนำข้อมูลเข้าสู่ Model เราจะต้องแบ่งข้อมูลออกเป็น 2 หรือ 3 ส่วนเสียก่อน ให้ลองนึกภาพว่าคุณเป็นนักแข่งจักรยานผาดโผน และคุณมีสนามสำหรับฝึกซ้อมอยู่ 1 สนาม คุณก็ฝึกหัดกับสนามของคุณจนคล่องแคล่ว คำถามมีอยู่ว่า คุณจะพร้อมสำหรับการแข่งขันหรือไม่ คำตอบที่ควรจะเป็น คือ ยังไม่พร้อม เพราะมีประสบการณ์กับสนามของตัวเองเท่านั้น ยังไม่เคยไปขี่ในสนามอื่นเลย เราเก่งในสนามของตัวเอง ส่วนหนึ่งอาจเป็นเพราะทักษะการขี่ แต่ส่วนหนึ่งเป็นเพราะเราจำได้ว่าสนามเป็นแบบไหน พอถึงจุดไหนต้องทำอย่างไร แต่เมื่อไปเจอสนามที่ไม่เคยขี่มาก่อน เราจะเหลือแค่ทักษะการขี่เท่านั้น พูดง่ายๆ คือ ขาดความหลากหลายของประสบการณ์\n",
    "<br>\n",
    "\n",
    "การฝึกสอน Model ก็เช่นเดียวกัน ถ้าเราเอาข้อมูลไปสอนก่อน จากนั้นเอาข้อมูลชุดเดิมไปทดสอบ Model ก็จะเก่งตามข้อมูลที่สอนและจำข้อมูลที่สอนได้ ในที่สุดก็อาจจะทำได้ดีมาก ถ้าเป็นข้อสอบก็อาจจะบอกว่าตอบถูกหมด แต่เมื่อไปเจอข้อมูลที่ไม่คุ้นเคย คราวนี้ก็จะทำได้ไม่ดี เราคงไม่ต้องการ Model ที่เป็นประเภท สิงห์สนามซ้อม แต่อ่อนสนามจริง ใช่มั้ย\n",
    "<br>\n",
    "\n",
    "ดังนั้นเมื่อจะ Train หรือฝึกสอน Model ก็จะมีการแบ่งข้อมูลออกเป็น 2 หรือ 3 ส่วน ประกอบด้วย\n",
    "- ข้อมูลสำหรับฝึกสอน (Training Set) ถ้าเปรียบเทียบกับการเรียนก็คือ งานที่ให้ในห้องเรียนหรือการบ้าน ข้อมูลส่วนนี้จะมีจำนวนมากที่สุด เพื่อฝึกให้มีความชำนาญหรือเก่ง โดยทั่วไปจะประมาณ 60-80% ของข้อมูล dataset ทั้งหมด\n",
    "- ข้อมูลสำหรับตรวจสอบความถูกต้อง (Validation Set) ถ้าเปรียบเทียบกับการเรียนก็คือ การ Quiz คือการทดสอบว่า Model เก่งแค่ไหนในขณะที่ทำการฝึกสอน ควรจะหยุดการฝึกสอนหรือยัง หรือ เปลี่ยนแปลงค่าการฝึกสอน โดยทั่วไปจะแบ่งข้อมูลประเภทนี้ไว้ระหว่าง 10-20% โดยข้อมูลนี้จะถูกนำมาทดสอบหลังจากการฝึกสอนในแต่ละ epoch\n",
    "- ข้อมูลสำหรับทดสอบจริง (Testing Set) ถ้าเปรียบเทียบกับการเรียน ก็คือ การสอบ Midterm หรือ Final Exam โดยการสอบนี้จะเป็นการทดสอบจริงว่าเมื่อนำ Model ไปใช้กับข้อมูลที่มันไม่เคยเห็นมาก่อน จะทำงานได้ดีหรือไม่ โดยทั่วไปจะแบ่งข้อมูลประเภทนี้ไว้ระหว่าง 10-20%\n",
    "\n",
    "ในการทำงาน อย่างน้อยจะต้องมี Training Set และ Testing Set เสมอจะขาดไม่ได้เลย แต่สำหรับ Validation Set ก็ขึ้นกับผู้พัฒนา Model ว่าจะใช้หรือไม่\n",
    "\n",
    "อาจมีข้อสงสัยว่า Validation Set กับ Testing Set ต่างกันอย่างไร ก็ขอตอบว่าทั้ง 2 ข้อมูลใช้ในการทดสอบเหมือนกัน แต่ Testing Set จะไม่นำข้อมูลไปใช้ในระหว่างขั้นตอนการ Train เลย พูดง่ายๆ คือ ต้องเป็นข้อมูลที่ไม่เคยเห็นมาก่อน (เหมือนสอบ Final) และจะทดสอบเพียงครั้งเดียว แต่ Validation Set จะนำมาทดสอบในทุกครั้งของรอบการฝึกสอน (Epoch) โดยไม่นำข้อมูลใน Validation Set ไปฝึกสอนด้วย แต่จะใช้ทดสอบว่าเรียนรู้ได้ดีหรือยัง ถ้าเรียนรู้ได้ดีผลงานใน Validation Set ก็จะดีไปด้วย\n",
    "\n",
    "เรื่องของข้อมูลทั้ง 3 ประเภทนี้ จะเห็นภาพชัดเจนขึ้นเมื่อเราเห็นภาพของการฝึกสอน\n",
    "\n",
    "\n",
    "\n",
    "| Split | Purpose | Amount of total data | How often is it used? |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **Training set** | The model learns from this data (like the course materials you study during the semester). | ~60-80% | Always |\n",
    "| **Validation set** | The model gets tuned on this data (like the practice exam you take before the final exam). | ~10-20% | Often but not always |\n",
    "| **Testing set** | The model gets evaluated on this data to test what it has learned (like the final exam you take at the end of the semester). | ~10-20% | Always |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXqX6Qb5CMp0"
   },
   "source": [
    "### ขั้นตอนการแบ่งข้อมูล\n",
    "\n",
    "ในส่วนนี้จะมี Library ที่เรียกใช้เพิ่มอีกหลายตัว\n",
    "- Pathlib จะมาช่วยในการจัดการไฟล์\n",
    "- tqdm ใช้ในการแสดงความก้าวหน้าในการ copy ไฟล์ เพื่อไม่ให้ดูเงียบๆ\n",
    "- shutil ใช้ในการ copy ไฟล์\n",
    "\n",
    "การแบ่งข้อมูลสำหรับโปรแกรมนี้ จะแบ่งออกเป็น 2 ส่วน คือ traning data กับ test data เท่านั้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "e07c2e785e43400d80c10691cf8db382",
      "24fcbb8a977146f58a9a27a3833c296a",
      "c438f1179a154b5eb752f9986e4d127a",
      "bf43f2e8af014cf4a67c27ffb39a7e8b",
      "0ad3c5748e81445d9e2407e12142c31d",
      "d2ccb6b6473947f3be0eef2c91fab70b",
      "fe8e645fce5f4df3833a0a45e73fded4",
      "e43cb91532fe4153b7565ef2e35902b9",
      "5be7368f19b9438baddd2f18ad1c5eae",
      "15a776dcd0dd407a8d8125cfd82be9ee",
      "5d432e78655248fcae6dc0b8dbf83f19",
      "4c92ec73b56b4b7e8d2926557822a34a",
      "8e99babf7f324a7b97e360134b6cf92a",
      "823824e590bd49c0bfef8ee84e85f22d",
      "8baeaec3cc93441597e76d19528a723b",
      "3ff6a10b199a46179025cb6c9248c10b",
      "5c6736ca59e1486f902a5ae8b7bb0006",
      "4e16a362c9d441e79f483d3a73215253",
      "f48426d7d2244d82b64ef94b271c9384",
      "361de2a68f8b478fb85c631ba11e041b",
      "29c3ed3fad894095ae2b773053177c78",
      "289c67385f884040982babd98f6f9dfe"
     ]
    },
    "executionInfo": {
     "elapsed": 2075,
     "status": "ok",
     "timestamp": 1720843713107,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "UcmhoBupddri",
    "outputId": "56907167-98e3-4bb0-e6b2-dbe6940526e3"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "train_paths, test_paths = train_test_split(all_file, test_size=0.1, random_state=42)\n",
    "\n",
    "for i in range(10):\n",
    "    os.makedirs(f\"data/train/{i}\", exist_ok=True)\n",
    "    os.makedirs(f\"data/test/{i}\", exist_ok=True)\n",
    "\n",
    "def copy_to_destination(src_paths, dst_path: str = \"data/train/\"):\n",
    "    for path in tqdm(src_paths):\n",
    "        path = Path(path)\n",
    "        if path.parent.name == \"10\":\n",
    "            parent_dir = \"0\"\n",
    "        else:\n",
    "            parent_dir = path.parent.name\n",
    "        shutil.copy(path, os.path.join(dst_path, parent_dir, path.name))\n",
    "\n",
    "copy_to_destination(train_paths, \"data/train/\")\n",
    "copy_to_destination(test_paths, \"data/test/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYHbKC__dYsk"
   },
   "source": [
    "### ลดขนาดข้อมูล\n",
    "\n",
    "เอาละ! มาถึงตรงนี้ เราก็มีข้อมูลพร้อมแล้ว โดยแบ่งเป็น training data จำนวน 1575 ข้อมูล และ test data จำนวน 175 ข้อมูล\n",
    "\n",
    "แต่ถ้ายังจำได้จะเห็นว่ารูปต้นฉบับมีขนาด 300x300 ซึ่งรูปที่มีขนาดใหญ่ จะทำให้การประมวลผลข้อมูลนาน ดังนั้นในกรณีนี้ เราควรจะลดขนาดภาพลง เพื่อให้การประมวลผลเร็วขึ้น แต่การลดขนาดภาพก็มีความเสี่ยงอยู่บ้างที่จะทำให้ผลลัพธ์แย่ลง\n",
    "\n",
    "ในการทำงานของ Nueral Network ให้เทียบกับการเรียนรู้ของมนุษย์ หากภาพมีข้อมูลมาก การมีภาพที่มีรายละเอียด (ขนาดใหญ่) ก็ทำให้มนุษย์ทำงานได้ดีขึ้น แต่ในบางกรณี เช่น กรณีนี้เราต้องการบอกว่าภาพเป็นภาพของเลขอะไรเท่านั้น ดังนั้นแม้จะลดความละเอียดลงไปบ้าง เราก็อาจจะแยกแยะได้อยู่ดี\n",
    "\n",
    "ดังนั้นในกรณีนี้ ถือว่ามีความคุ้มค่าในการลดขนาดภาพ เพราะประสิทธิภาพการทำงานไม่ลดลงมากนัก แต่ความเร็วลดลงได้มาก\n",
    "\n",
    "ในการลดขนาดข้อมูลกรณีนี้จะลดเหลือแค่ 28x28 เท่านั้น โดยค่า 28x28 เป็นค่าที่ได้จากการทดลองในอดีตที่พบว่ารูปขนาดเล็กที่ 28x28 ให้ผลดีมากกว่า 24x24 หรือ 30x30\n",
    "\n",
    "ในการลดขนาดข้อมูล เราจะใช้ library อีกตัวหนึ่งซึ่งผู้พัฒนา PyTorch ได้พัฒนาไว้ โดยมีชื่อว่า TorchVision ซึ่งทำหน้าที่เกี่ยวกับการประมวลผลภาพ โดยใน TorchVision จะมี Module หนึ่งชื่อว่า transform ซึ่งทำหน้าที่แปลงภาพ โดยการแปลงนี้จะมี 3 ขั้นตอน คือ\n",
    "- แปลงจาก 300x300 เป็น 28x28\n",
    "- เปลี่ยนเป็นภาพ Grayscale\n",
    "- เปลี่ยนเป็น Tensors ของ Torch และเปลี่ยนจาก 0-255 เป็น 0-1\n",
    "\n",
    "จากภาพที่แสดง จะเห็นได้ชัดเจนว่าภาพมีรายละเอียดลดลงอย่างชัดเจน แต่ก็ยังเห็นว่าเป็นเลข 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW5gF3OQdxfx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1720752131856,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "e99KC-FhlZ0D",
    "outputId": "96b3121c-bcdc-4e8c-f1d4-26bde3215f3a"
   },
   "outputs": [],
   "source": [
    "im = Image.open(all_file[100])\n",
    "im = (1 - transform(im)).squeeze(0)\n",
    "print(im[10])\n",
    "plt.title(\"Number: {}\".format(Path(all_file[100]).parent.name))\n",
    "plt.imshow(im, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tumYdtIIjeXR"
   },
   "source": [
    "ในบรรทัด im = (1 - transform(im)).squeeze(0) มีการทำงาน 2 อย่าง คือ\n",
    "- squeeze ซึ่งทำหน้าที่ลดมิติของ im ให้เหลือเพียงมิติเดียว\n",
    "- 1 - transform(im) ทำหน้าที่กลับสีระหว่างดำกับขาว เพื่อให้ประมวลผลได้ง่ายขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MLgujPF1l7y5"
   },
   "source": [
    "### สร้าง class ของข้อมูล\n",
    "\n",
    "เวลานี้เรามีข้อมูลที่แบ่งออกเป็น Training Data และ Test Data แล้วแต่ข้อมูลที่เป็นไฟล์จะยุ่งยากต่อการจัดการ ดังนั้นหลักนิยมเขามักจะสร้าง \"ตัวครอบ\" ขึ้นมาอีกชั้น โดยมอง ตัวครอบ ที่ว่าเป็นแหล่งข้อมูล Data Source โดยไม่สนใจว่าข้อมูลนั้นจะเก็บไว้ที่ใด ซึ่ง PyTorch ก็ได้เตรียม Library เอาไว้ สำหรับใช้ในงานนี้ โดยอยู่ในโมดูล Dataset\n",
    "- ใน `__init__` จะเป็นการกำหนดข้อมูลเบื้องต้นของคลาส ได้แก่ directory หรือ folder ของไฟล์รูปภาพ และ การ transform (ถ้ามี) และสร้าง image label จากข้อมูลไฟล์\n",
    "- ใน `__len__` จะทำให้สามารถใช้คำสั่ง len กับข้อมูลได้\n",
    "- ใน `__getitem__` จะทำให้สามารถอ้างถึงข้อมูลในรูปแบบคล้าย List ได้ดังตัวอย่าง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbz6iDYJmuja"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ThaiHandNumData(Dataset):\n",
    "    def __init__(self, image_dir: str, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_labels = [(image_dir, Path(image_dir).parent.name) for image_dir in glob(os.path.join(image_dir, \"*\", \"*\"))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_labels[idx]\n",
    "        label = int(label)\n",
    "        image = Image.open(image)\n",
    "        if self.transform:\n",
    "            image = 1 - self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5RtAyj3vXbc"
   },
   "source": [
    "จากนั้นทำการสร้าง object ข้อมูล ขึ้นมา 2 object ชื่อว่า training_data และ validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1720844502585,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "R1_lHtiom2dV",
    "outputId": "8f5ca6f3-7902-494a-8903-e5a90a7ad1f1"
   },
   "outputs": [],
   "source": [
    "training_data = ThaiHandNumData(\"data/train/\", transform=transform)\n",
    "validation_data = ThaiHandNumData(\"data/test/\", transform=transform)\n",
    "\n",
    "image, label = training_data[100]\n",
    "plt.imshow(image.permute(1, 2, 0), cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjluWGqWv1EO"
   },
   "source": [
    "ต่อไปจะเป็นการสร้างสิ่งที่เรียกว่า batch of data เนื่องจากในการทำงานนี้เราจะใช้วิธีการที่เรียกว่า mini batch ในการปรับ weight กล่าวคือ\n",
    "- จะไม่ปรับ weight กับทุกข้อมูลเนื่องจากจะทำให้ช้า\n",
    "- จะไม่ปรับ weight ครั้งเดียวตอนครบทุกข้อมูลเพราะประสิทธิภาพจะไม่ดี\n",
    "\n",
    "ดังนั้นเราจะใช้วิธีการแบ่งข้อมูลออกเป็นส่วนๆ แล้วป้อนเข้าไปใน Model เช่น ส่งไปครั้งละ 16 ข้อมูล หรือ 32 ข้อมูล จากนั้นเมื่อหาค่า Loss แล้ว จึงนำมาปรับ weight เป็นครั้งๆ ไป ซึ่งจากการทดลองพบว่า จะให้ผลดีกว่าการปรับ weight กับทุกข้อมูล หรือ ปรับ weight ครั้งเดียวตอนครบทุกข้อมูล นอกจากนั้นการส่งข้อมูลเข้าไปเป็น batch ยังทำให้ลดการใช้หน่วยความจำใน GPU อีกด้วย\n",
    "\n",
    "<img src=\"https://static.javatpoint.com/tutorial/machine-learning/images/epoch-in-machine-learning2.png\" width=\"400\"/><div>\n",
    "\n",
    "\n",
    "link ด้านล่างนี้เป็น link Twitter(X) ของ Yann LeCun ปรมาจารย์คนหนึ่งในวงการ AI ซึ่งเป็นหัวหน้านักวิทยาศาสตร์เอไอที่บริษัท Meta เขาบอกว่าไม่ควรจะให้ Batch Size มากกว่า 32\n",
    "\n",
    "[Yann LeCun : say about batch  size](https://x.com/ylecun/status/989610208497360896)\n",
    "\n",
    "รูปด้านล่างแสดงผลของการปรับ weight ในแต่ละแบบ\n",
    "- แบบ Batch Gradient Descent  การปรับค่าพารามิเตอร์จะเกิดขึ้นโดยใช้ข้อมูลทั้งหมดในหนึ่งชุดข้อมูล (batch) ต่อการอัปเดตหนึ่งครั้ง วิธีการนี้ทำให้เกิดการอัปเดตที่มีเสถียรภาพ เนื่องจากการคำนวณ gradient จะทำจากข้อมูลทั้งหมด ทำให้ค่า gradient มีค่าที่แน่นอนและไม่เปลี่ยนแปลงบ่อยครั้ง ข้อเสีย คือ เสียเวลาและทรัพยากรมาก  จึงเหมาะสำหรับปัญหาที่มีชุดข้อมูลขนาดเล็กถึงขนาดกลาง\n",
    "\n",
    "- แบบ Stochastic Gradient Descent การปรับค่าพารามิเตอร์จะเกิดขึ้นโดยใช้ข้อมูลเพียงหนึ่งตัวอย่าง (instance) ต่อการอัปเดตหนึ่งครั้ง แทนที่จะใช้ข้อมูลทั้งหมดในชุดข้อมูล (batch) เหมือนใน Batch Gradient Descent (เลือกข้อมูลโดยการสุ่ม) ทำให้การอัปเดตพารามิเตอร์เกิดขึ้นบ่อยครั้งและมีการเปลี่ยนแปลงที่หลากหลายมากขึ้น ซึ่งสามารถช่วยหลีกเลี่ยง local minima ได้ดีขึ้นในบางกรณี\n",
    "\n",
    "- แบบ Mini-Batch Gradient Descent คือวิธีการปรับค่าพารามิเตอร์ของโมเดลในการเรียนรู้เชิงลึกหรือการเรียนรู้ของเครื่องที่ผสมผสานข้อดีของ Batch Gradient Descent และ Stochastic Gradient Descent โดยแบ่งชุดข้อมูลทั้งหมดออกเป็นกลุ่มย่อยๆ (mini-batches) แล้วใช้แต่ละกลุ่มย่อยในการคำนวณ gradient และอัปเดตพารามิเตอร์ของโมเดล\n",
    "\n",
    "<img src=\"https://statusneo.com/wp-content/uploads/2023/09/Credit-Analytics-Vidya.jpg\" width=\"600\"/><div>\n",
    "<br>\n",
    "\n",
    "ในโปรแกรมของเราจะใช้แบบ mini-batch และใช้ batch size เป็น 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "US4T7sMwm266"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_loader = DataLoader(training_data, batch_size=16, shuffle=True)\n",
    "validation_loader = DataLoader(validation_data, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1720845119716,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "Iv6bVbdKJ4tT",
    "outputId": "2b748205-8dff-4203-e32c-6b3e787c2c77"
   },
   "outputs": [],
   "source": [
    "# load example batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(images.shape, labels.shape)\n",
    "plt.imshow(image.permute(1, 2, 0), cmap='gray')\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1720845128528,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "tvyssS4lKN5i",
    "outputId": "1c8d174b-581f-453d-f21d-5b10f4c7a823"
   },
   "outputs": [],
   "source": [
    "# print(image[0])\n",
    "images.shape, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFi94Hu8cIgo"
   },
   "source": [
    "## การสร้าง Model\n",
    "\n",
    "ก่อนจะสร้างข้อมูลขอทบทวนขั้นตอนการ Train อีกครั้งโดยมีขั้นตอนตามตารางนี้ ดังนั้นในการ Train ให้ตรวจสอบว่าทำครบในทุกขั้นตอน\n",
    "\n",
    "| Number | Step name | What does it do? | Code example |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| 1 | Forward pass | The model goes through all of the training data once, performing its `forward()` function calculations. | `model(x_train)` |\n",
    "| 2 | Calculate the loss | The model's outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are. | `loss = loss_fn(y_pred, y_train)` |\n",
    "| 3 | Zero gradients | The optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step. | `optimizer.zero_grad()` |\n",
    "| 4 | Perform backpropagation on the loss | Computes the gradient of the loss with respect for every model parameter to be updated  (each parameter with `requires_grad=True`). This is known as **backpropagation**, hence \"backwards\".  | `loss.backward()` |\n",
    "| 5 | Update the optimizer (**gradient descent**) | Update the parameters with `requires_grad=True` with respect to the loss gradients in order to improve them. | `optimizer.step()` |\n",
    "<br>\n",
    "\n",
    "ขั้นตอนแรก คือ forward pass ซึ่ง forward pass จะสร้างใน Class ของ Model Neural Network โดยในที่นี้ตั้งชื่อว่า ThaiNumberNN ซึ่งแสดงตามโปรแกรมด้านล่าง\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAAZiooU20kp"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ThaiNumberNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 80)\n",
    "        self.fc2 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkZ1apUxTiu8"
   },
   "source": [
    "จะเห็นว่าใน Model จะประกอบด้วย Hidden Layer จำนวน 1 Layer โดย Input มีขนาดเป็นเป็น 28x28 เนื่องจากในแต่ละรูปมีจุดภาพ 28x28 จุด โดยแต่ละจุดภาพมีขนาด 1 ไบต์ (เนื่องจากเก็บแบบ Grayscale) ดังนั้นเราจะนำข้อมูลแต่ละจุดมาเรียงกันเป็น Array 1 มิติ (ขนาด 784 ไบต์) โดยผ่านคำสั่ง view\n",
    "\n",
    "แล้วส่งเข้าในแต่ละ Neuron ของ Hidden Layer ชั้นที่ 1 ซึ่งมีจำนวน 80 Neuron และนำมาผ่านฟังก์ชัน ReLU แล้วจึงนำเข้ามาที่ output layer ซึ่งจะมี 10 neuron ซึ่งเท่ากับจำนวน Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 377,
     "status": "ok",
     "timestamp": 1720846470511,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "f7BHye6PpxKw",
    "outputId": "61f2db49-078f-4585-af4a-116f7cdb439a"
   },
   "outputs": [],
   "source": [
    "my_NN = ThaiNumberNN()\n",
    "images, labels = next(iter(train_loader))\n",
    "pred = my_NN(images)\n",
    "print(images.shape, pred.shape)\n",
    "print(pred[:5])\n",
    "print(pred.argmax(1))\n",
    "labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmt-AhkY3Es5"
   },
   "source": [
    "### ส่วน Train ข้อมูล และหาค่า Validation\n",
    "\n",
    "คราวนี้ก็มาถึงการ train ข้อมูล ซึ่งหากดูจากตาราง จะพบว่าประกอบด้วยขั้นตอน 1) calculate loss 2) zero gradient 3) backpropagation และ 4) update parameter ผ่าน optimizer function\n",
    "\n",
    "ในฟังก์ชัน train จะมีส่วนประกอบดังนี้\n",
    "- training_logs ทำหน้าที่เก็บข้อมูลจำนวน 4 ข้อมูล คือ training loss ของแต่ละรอบ, training accuracy ของแต่ละรอบ, validation loss และ validation accuracy โดย loss จะเป็นผลรวมค่า loss ของทุกข้อมูล (training loss คือ ผลรวมของ training data, validation loss ผลรวมของ validation data) สำหรับ accuracy คือ ร้อยละที่ทำนายได้ถูกของแต่ละข้อมูล\n",
    "- ใน Loop นอกจะทำหน้าที่รัน epoch คือ ส่งข้อมูลเข้าไป train จนครบทุกข้อมูล\n",
    "- เริ่มต้นจะกำหนดให้เป็น training mode โดยใช้คำสั่ง `model.train()`\n",
    "- ส่วน loop ด้านใน loop แรก `for images, labels in train_loader`  จะทำหน้าที่ค่อยๆ นำข้อมูลของ image และ label ของรูป ออกมาทีละรูป\n",
    "- `pred = model(images)` นำรูปนั้นไปผ่าน forward pass ของ model\n",
    "- `loss = loss_function(pred, labels)` คำนวณค่า loss เทียบกับ label โดยใช้ loss_function\n",
    "- `optimizer.zero_grad()` clear ค่า gradient ที่คำนวณได้ออก เพื่อเตรียมสำหรับรันใน loop ถัดไป\n",
    "- `loss.backward()` ทำ Backpropagation เพื่อคำนวณค่าที่ต้องใช้ในการปรับพารามิเตอร์\n",
    "- `optimizer.step()` ทำการปรับพารามิเตอร์\n",
    "- บันทึกค่า loss และ ค่าความถูกต้อง valid\n",
    "\n",
    "จากนั้นก็เก็บค่า Loss และ Validation ของ training รอบนั้น คราวนี้ต่อไปก็จะมาทำส่วนของ Validation Test ซึ่งขั้นตอนก็จะคล้ายกับ Training เพียงแต่จะไม่มีการปรับพารามิเตอร์ จะเห็นว่าคำสั่ง `optimizer.zero_grad()`, `loss.backward()` และ `optimizer.step()` ไม่มีใน loop นี้ ซึ่งก็คือ การนำข้อมูลอีกชุดมาตรวจสอบผลการทำงานนั่นเอง\n",
    "\n",
    "จากนั้นในทุกๆ 5 รอบก็จะพิมพ์ผลมาดูครั้งหนึ่ง (ปรับเป็นค่าอื่นก็ได้)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 960
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "error",
     "timestamp": 1720943413173,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "viCAluFD3IZF",
    "outputId": "e5aeb748-189b-4978-a721-4b0ecceef162"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create training loop function\n",
    "def train(model, n_epochs, loss_function, optimizer, train_loader, test_loader):\n",
    "    training_logs = {\"train_loss\": [],  \"train_acc\": [], \"validate_loss\": [], \"validate_acc\": []}\n",
    "    print(\"-\"*80)\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # training\n",
    "        model.train()\n",
    "        train_loss, correct = 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "            pred = model(images)\n",
    "            loss = loss_function(pred, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            correct += (pred.argmax(1) == labels).float().sum().item()\n",
    "        # save training logs\n",
    "        training_logs[\"train_loss\"].append(train_loss/ len(train_loader))\n",
    "        training_logs[\"train_acc\"].append(correct / len(train_loader.dataset))\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        test_loss, correct = 0, 0\n",
    "        for images, labels in validation_loader:\n",
    "            images, labels = images.to(device), labels.to(device)  # Move data to GPU\n",
    "            pred = model(images)\n",
    "            test_loss += loss_function(pred, labels).item()\n",
    "            correct += (pred.argmax(1) == labels).float().sum().item()\n",
    "        # save validation logs\n",
    "        training_logs[\"validate_loss\"].append(test_loss/ len(validation_loader))\n",
    "        training_logs[\"validate_acc\"].append(correct / len(validation_loader.dataset))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "          print(f\"Epochs {epoch}\".ljust(10),\n",
    "                f\"train loss {training_logs['train_loss'][-1]:.5f}\",\n",
    "                f\"train acc {training_logs['train_acc'][-1]:.5f}\",\n",
    "\n",
    "                f\"validate loss {training_logs['validate_loss'][-1]:.5f}\",\n",
    "                f\"validate acc {training_logs['validate_acc'][-1]:.5f}\",\n",
    "                )\n",
    "          print(\"-\"*80)\n",
    "    return model, training_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VY2nifr_zbUG"
   },
   "source": [
    "### ส่วน run model\n",
    "\n",
    "คราวนี้ก็มาถึงส่วนที่ทำหน้าที่ run model โดยกำหนด\n",
    "- ค่า epoch ไว้ที่ 100 รอบ\n",
    "- ใช้ loss function เป็น `nn.CrossEntropyLoss()` เพราะใช้กับการทำงานแบบ Classification\n",
    "- ใช้ gradient descent หรือการปรับพารามิเตอร์แบบไล่ตามความชันในแบบ SDG หรือ Stocastic Gradient Descent\n",
    "- กำหนด learning rate เป็น 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 483435,
     "status": "ok",
     "timestamp": 1720846964829,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "S_IW7-oj3RNP",
    "outputId": "3ab38bc5-fa86-4cf4-8ec6-dd2d699e7fb3"
   },
   "outputs": [],
   "source": [
    "n_epoch = 100\n",
    "\n",
    "TN_model = ThaiNumberNN().to(device) # Initialize the model\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(TN_model.parameters(), lr=0.01)\n",
    "\n",
    "TN_model, TN_model_history = train(TN_model, n_epoch, loss_fn, optimizer, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UT_Op9Q3BRi"
   },
   "source": [
    "### ผลการทำงาน\n",
    "\n",
    "- จะเห็นได้ว่าเมื่อทำงานไปเรื่อยๆ จะพบว่าค่าความถูกต้องของ training accuracy จะเพิ่มขึ้นเรื่อย จนค่าสุดท้ายคือ 91 % ซึ่งถือว่าดีมากทีเดียว\n",
    "- แต่เมื่อนำข้อมูลที่กันเอาไว้สำหรับตรวจสอบความถูกต้อง มาทดสอบก็พบว่าความถูกต้องจะอยู่ที่ประมาณ 60% กว่ามาตั้งแต่รอบที่ 25 และไม่ดีขึ้นเลย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L9tZdiI1F7bj"
   },
   "outputs": [],
   "source": [
    "def plot_graph(history):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_figwidth(10)\n",
    "    fig.suptitle(\"Train vs Validation\")\n",
    "    ax1.plot(history[\"train_acc\"], label=\"Train\")\n",
    "    ax1.plot(history[\"validate_acc\"], label=\"Validation\")\n",
    "    ax1.legend()\n",
    "    ax1.set_title(\"Accuracy\")\n",
    "\n",
    "    ax2.plot(history[\"train_loss\"], label=\"Train\")\n",
    "    ax2.plot(history[\"validate_loss\"], label=\"Validation\")\n",
    "    ax2.legend()\n",
    "    ax2.set_title(\"Loss\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1720846980893,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "lMikUdTmGQ6N",
    "outputId": "05294b1e-daee-4d33-ee76-e93491a030ac"
   },
   "outputs": [],
   "source": [
    "plot_graph(TN_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOTl3pVO4ffG"
   },
   "source": [
    "### วิเคราะห์กราฟ\n",
    "\n",
    "เมื่อนำข้อมูลของ Accuracy มา plot เป็นกราฟก็จะเห็นได้ชัดเจนว่าหลังจากรอบที่ 20 ความถูกต้องเมื่อเทียบกับ validation data ไม่ได้เพิ่มขึ้นเท่าไร แต่ความถูกต้องของ traning data กลับเพิ่มขึ้นเรื่อยๆ\n",
    "\n",
    "การฉีกออกจากกันระหว่าง กราฟ traing accuracy และ validation accuracy แบบนี้แสดงให้เห็นว่าการ train ยังไม่ได้ผลที่ดีมากนัก โดยการ train ที่ได้ผลดี กราฟ trainng accuracy และ validation accuracy ควรจะอยู่ใกล้ๆ กัน ไม่แยกออกจากกันมาก\n",
    "<br>\n",
    "\n",
    "#### Overfitting\n",
    "การแยกออกจากกันของ validation accuracy กับ trainng accuracy เป็นปัญหาที่เรียกว่า **overfitting** ซึ่งเป็นสถานการณ์ที่เกิดขึ้นเมื่อโมเดล machine learning ทำงานได้ดีมากกับข้อมูลการฝึก (training data) แต่ทำงานได้ไม่ดีหรือแย่มากกับข้อมูลทดสอบ (testing data) หรือข้อมูลที่ไม่เคยเห็นมาก่อน โดยลักษณะคือ Training Accuracy สูง แต่ Testing Accuracy หรือ validation Accuracy ต่ำ\n",
    "\n",
    "> <img src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*lARssDbZVTvk4S-Dk1g-eA.png\" width=\"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "#### เปรียบเทียบกับการทำงานของสมอง\n",
    "\n",
    "การ overfit หากเปรียบเทียบกับการทำงานของสมอง ก็ต้องบอกว่าเป็นการใช้ความจำมากเกินไป เช่น สมมติว่าเราซ้อมจักรยานวิบาก ในสนามที่ซ้อมเป็นประจำ จนรู้ว่าตรงไหนเป็นหลุม หรือเป็นเนิน ทำให้เราสามารถหลบเนินหรือหลุมได้ ทำให้เราขับได้คล่องในสนามที่ซ้อมเป็นประจำ แต่ทำให้เราขาดทักษะในการปรับตัวกับสนามแบบทั่วไป ดังนั้นแทนที่จะพัฒนาทักษะหลบหลุม หรือ เนิน กลับไปใช้ความจำแทน\n",
    "\n",
    "ในทาง Machine Larning อาจจะบอกว่าการเรียนรู้ไปจดจำ Noise มากเกินไป ทำให้พลาดการมองแนวโน้ม"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YP3MhLfn5or-"
   },
   "source": [
    "### การนำไปใช้งาน\n",
    "\n",
    "ทีนี้สมมติว่าเราต้องการเอาข้อมูลมาทดสอบสักหน่อย จะทำอย่างไร เราก็ต้องเขียนโปรแกรมอ่านภาพ และนำมา transform จากนั้นนำไป predict และเทียบผลกับคำตอบจริง\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1720269115224,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "ZBqMQvuy5w8t",
    "outputId": "0d055308-311f-4cf2-f71c-45a3482ceabf"
   },
   "outputs": [],
   "source": [
    "# Show some random images with their predicted number\n",
    "sample_path = glob(\"data/test/*/*.png\")[99]\n",
    "img = Image.open(sample_path)\n",
    "img = (1 - transform(img)).squeeze(0).to(device)\n",
    "TN_model.to(device)\n",
    "pred = TN_model(img)\n",
    "pred = int(pred.argmax(dim=1).item())\n",
    "true_class = Path(sample_path).parent.name\n",
    "\n",
    "plt.title(\"Predicted class = {}, True class = {}\".format(pred, true_class))\n",
    "plt.imshow(img.cpu(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgLY140G6l8Y"
   },
   "source": [
    "## Optimization Algorithm\n",
    "\n",
    "ใน Machine Learning มี Optimization Algorithm อยู่มากมาย เช่น หากอยากจะทราบว่าใน PyTorch มี optimization มีอะไรบ้างให้คลิก [ที่นี่](https://pytorch.org/docs/stable/optim.html#algorithms)\n",
    "\n",
    "ต่อไปจะอธิบาย Optimization Algorithm อย่างย่อๆ จำนวน 3 ตัวได้แก่\n",
    "- Stochastic Gradient Descent (SGD) optimizer\n",
    "- Adam Optimizer\n",
    "- SDG with Momentum\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nT3TosFj7qyl"
   },
   "source": [
    "### Stochastic Gradient Descent (SGD)\n",
    "\n",
    "- Gradient Descent (GD): เป็นวิธีการหาค่าต่ำสุดของ loss function  โดยการอัพเดตพารามิเตอร์ในทิศทางที่ทำให้ Loss มีค่าลดลงไปเรื่อยๆ ซึ่งสามารถทำได้โดยการคำนวณ gradient  ของ loss function ตามพารามิเตอร์แล้วอัพเดตพารามิเตอร์ไปในทิศทางที่ตรงข้ามกับ gradient นั้น วิธีการนี้ให้ที่ถูกต้องสูง แต่จะเสียเวลาในการทำงาน เนื่องจากจะต้องอัพเดตพารามิเตอร์จากทุกข้อมูล\n",
    "\n",
    "- Stochastic Gradient Descent (SGD): แตกต่างจาก GD ตรงที่การคำนวณ gradient ในแต่ละครั้งจะไม่ใช้ข้อมูลทั้งหมด แต่จะใช้ข้อมูลตัวอย่างเดียวหรือชุดย่อย (mini-batch) ของข้อมูลเท่านั้น ซึ่งทำให้การคำนวณ gradient ในแต่ละครั้งเร็วขึ้น ซึ่งจะเกิดผลทำให้การปรับพารามิเตอร์มีการ swing ตามรูปที่แสดงอยู่ด้านบน\n",
    "\n",
    "*** SDG กับ local minima ***\n",
    "\n",
    "- เนื่องจาก gradient ของทุกพารามิเตอร์ อาจมีจุดต่ำสุดหลายที่ตามรูป โดยจุดต่ำสุดเฉพาะถิ่น จะเรียกว่า local minima และจุดต่ำสุดของทั้งหมดจะเรียกว่า global minima\n",
    "- โดยในการทำงานแบบ gradient descent นั้น การสุ่มตำแหน่งแรกของการทำงานเพื่อนำมาคำนวณค่า gradient นั้นมีความสำคัญมาก เพราะหากเริ่มต้นจากจุดที่ไม่ดีแล้ว อาจทำให้ได้จุดต่ำสุดที่ไม่ใช่จุดต่ำสุดรวม\n",
    "- เนื่องจาก SDG ใช้การสุ่มข้อมูลมาคำนวณค่า gradient เป็นผลทำให้เกิดการกระโดดของจุดทำงาน ซึ่งอาจบังเอิญเป็นการกระโดดข้าม local minima ก็ได้ ดังนั้น SDG จึงได้รับความนิยมมากกว่า และ ยังมีการทำงานที่เร็วกว่าอีกด้วย\n",
    "\n",
    "> <img src=\"https://www.researchgate.net/publication/338621083/figure/fig4/AS:847811214069760@1579145353037/Gradient-Descent-Stuck-at-Local-Minima-18.ppm\" width=\"300\"/><div>\n",
    "<br>\n",
    "\n",
    "> <img src=\"https://miro.medium.com/v2/resize:fit:1400/0*HwFeAB4nHLpqgw-7.png\" width=\"400\"/><div>\n",
    "<br>\n",
    "\n",
    "*Ref : Loss landscape of a convolutional neural network with 56 layers (VGG-56) [source](https://arxiv.org/abs/1712.09913)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GngiDWH_4vo"
   },
   "source": [
    "### SDG with Momentum\n",
    "\n",
    "- SGD with Momentum เป็นเทคนิคที่ปรับปรุงจาก Stochastic Gradient Descent (SGD) โดยการเพิ่ม \"momentum\" เพื่อเร่งการลู่เข้า (convergence) และลดการสั่น (oscillations) ของการปรับพารามิเตอร์\n",
    "- โดยการปรับพารามิเตอร์ไม่เพียงแต่พิจารณาจาก gradient ของ loss function ในปัจจุบันเท่านั้น แต่ยังพิจารณาจากทิศทางของการอัพเดตก่อนหน้านี้ด้วย ซึ่งเป็นที่มีของคำว่า momentum\n",
    "- ขอยกตัวอย่างเช่น สมมติว่าการปรับพารามิเตอร์ เป็นไปในทิศทางหนึ่งมาตลอด แต่อยู่ๆ ก็มีการฉีกออกจากเส้นทางเดิม เนื่องจากการสุ่มข้อมูลที่นำมาคำนวณ gredient อาจเป็นข้อมูลที่มีความต่างไปจากข้อมูลอื่นๆ มาก ก็จะนำค่า momentum มาลบออกจากค่าของการปรับ ทำให้การฉีกจากเส้นทางเดิมมีค่าลดลง\n",
    "\n",
    "ใน PyTorch ใช้ SDG with Momentum ดังนี้\n",
    "```\n",
    "# optimizer SGD with Momentum\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdI6Evk6Cklz"
   },
   "source": [
    "### Adam (Adaptive Moment Estimation)\n",
    "\n",
    "ก่อนจะทำความเข้าใจกับ Adam ต้องรู้จักกับ Adagrad และ RMSProp ก่อน\n",
    "- Adagrad (Adaptive Gradient Algorithm) Adagrad เป็นอัลกอริธึมการปรับค่าเรียนรู้ (learning rate) แบบปรับตัวเองได้ ที่พัฒนาขึ้นมาเพื่อจัดการกับปัญหาการลดค่าเรียนรู้ในระหว่างการฝึกโมเดล machine learning อัลกอริธึมนี้ได้รับความนิยมเพราะสามารถปรับค่าเรียนรู้ของพารามิเตอร์แต่ละตัวได้อย่างอัตโนมัติ ทำให้เหมาะสมกับข้อมูลและการอัพเดตที่แตกต่างกันในแต่ละพารามิเตอร์ Adagrad ปรับค่าเรียนรู้ของพารามิเตอร์แต่ละตัวโดยพิจารณาจากประวัติของ gradient ก่อนหน้า แนวคิดหลักคือการใช้ gradient ที่เคยคำนวณในอดีตเพื่อปรับค่าเรียนรู้ในปัจจุบัน โดยการปรับค่านี้จะทำให้พารามิเตอร์ที่มีการอัพเดตมากมีค่าเรียนรู้ที่ลดลง และพารามิเตอร์ที่มีการอัพเดตน้อยจะมีค่าเรียนรู้ที่สูงขึ้น สำหรับการทำงานโดยละเอียด จะไม่อธิบาย ผู้สนใจให้ศึกษาเพิ่มเติม\n",
    "- RMSprop (Root Mean Square Propagation) RMSprop เป็นอัลกอริธึมการปรับค่าพารามิเตอร์ที่ใช้ในการฝึกโมเดล machine learning ซึ่งเป็นการพัฒนาต่อจาก Adagrad โดยปรับปรุงปัญหาที่ค่า learning rate ลดลงเร็วเกินไป RMSprop มีแนวคิดหลักคือการรักษาค่า learning rate ให้เหมาะสมตลอดการฝึกโดยใช้ค่าเฉลี่ยเคลื่อนที่ (moving average) ของกำลังสองของ gradient ซึ่งช่วยลดปัญหาที่ค่า learning rate ลดลงเร็วเกินไปใน Adagrad\n",
    "\n",
    "รูปด้านล่างแสดงการเปรียบเทียบการทำงานระหว่าง SDG, Momentum, Adagrad, RMSprop และอื่นๆ ซึ่งจะเห็นได้ว่าในขณะที่ SDG ติดอยู่ที่ local minima แต่ตัวอื่นสามารถออกจาก local minima ได้\n",
    "\n",
    "<img src=\"https://editor.analyticsvidhya.com/uploads/121381obtV.gif\" width=\"400\"/><div>\n",
    "<br>\n",
    "\n",
    "สำหรับ ADAM เป็นการนำเอาอัลกอริทึม Adagrad และ RMSprop มารวมคุณสมบัติที่ดีไว้ด้วยกัน ซึ่งไม่อธิบายการทำงานโดยละเอียดเช่นกัน\n",
    "\n",
    "ในเรื่องของ Optimization ไม่มีอัลกอริทึมใดที่ดีที่สุด เช่น ในการประมาณค่าสมการเส้นตรงแบบที่เราทำในตอนแรก หากนำ Adam ไปใช้กลับให้ผลการทำงานที่ไม่ดี ดังนั้นต้องเลือกใช้ อัลกอริทึม  Optimization ให้เหมาะสม ซึ่งจะทดลองเองก็ได้ หรือศึกษางานของผู้อื่น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoGK0QrlfXfM"
   },
   "source": [
    "## การปรับปรุง Model (Optimization)\n",
    "\n",
    "มาถึงจุดนี้ คงจะเห็นภาพรวมของการทำงานของ Machine Learning แล้ว ซึ่งคงยังจำกันได้ว่า Model ที่เราสร้างยังทำงานได้ไม่ดี เนื่องจากเกิด Overfitting ขึ้น\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OQ3V_FFKUF2"
   },
   "source": [
    "### Overfitting คืออะไร\n",
    "\n",
    "เนื่องจาก Overfitting คือ สิ่งหนึ่งที่แสดงว่า Model ยังไม่ดี ดังนั้นเราจะมากทำความเข้าใจกันว่า Overfitting คืออะไร ก่อนหน้านี้ได้กล่าวว่า Overfitting คือ \"การเรียนรู้ไปจดจำ Noise มากเกินไป ทำให้พลาดการมองแนวโน้ม\"\n",
    "\n",
    "ลองมาดูรูปนี้\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/format:webp/1*SJENTprShSaT9eB-4CUyAw.gif\" width=\"300\"/><div>\n",
    "\n",
    "\n",
    "จากรูปจะเห็นการเปลี่ยนไปของ weight ในระหว่างการ train จะเห็นว่าบางเส้นทางค่า weight มีค่าเพิ่มขึ้น แปลว่า model ให้ค่ากับพารามิเตอร์ หรือ ข้อมูลของเส้นทางนั้นมาก คราวนี้ลองตอบคำถาม 2 ข้อ\n",
    "\n",
    "**คำถามข้อที่ 1 **\n",
    "- ค่า Loss ต่ำสุดของ Model มีได้กี่แบบ\n",
    "- คำตอบ คือ มีได้มากมาย เนื่องจากค่าพารามิเตอร์เริ่มต้นจะเป็นค่าสุ่ม และการเลือกตัวอย่างสำหรับการปรับพารามิเตอร์ก็เป็นการสุ่ม ดังนั้นเมื่อ run model ไปเรื่อยๆ ในที่สุดก็จะได้ชุดพารามิเตอร์ชุดหนึ่งที่ทำให้ค่า loss ต่ำ และ หากรันอีกครั้งก็จะได้ค่าพารามิเตอร์ที่ทำให้ค่า loss ต่ำใกล้เคียงกับกับครั้งแรก ดังนั้นสรุปได้ว่าผลจากการ run model จะมีคำตอบมากมาย แต่ได้ผลสิ่งเดียวกัน คือ ทำให้ค่า loss ต่ำ ทำให้สามารถทำนาย หรือ ประมาณค่าได้ถูกต้อง\n",
    "\n",
    "**คำถามข้อที่ 2 **\n",
    "- ระหว่าง model ที่มีผลเดียวกัน แต่มีค่าพารามิเตอร์แตกต่างกันมาก คือ ค่ามากก็มาก ค่าน้อยก็น้อย กับอีก model ที่มีพารามิเตอร์แตกต่างกันน้อยกว่า ควรเลือก model ไหนมาใช้\n",
    "- คำตอบ ควรเลือกโมเดลที่ 2 หากถามว่าทำไม ก็ต้องย้อนความทรงจำกลับไปที่เรื่องการประมาณค่า sine ซึ่งจะเห็นว่าแต่ละ neuron จะ active กับแต่ละข้อมูลไม่เท่ากัน และ มีความจริงอีกประการ คือ ข้อมูลที่นำมาให้ model เรียนรู้ บางข้อมูลจะใกล้เคียงกัน แต่บางข้อมูลจะต่างไปจากข้อมูลอื่นๆ มาก (ซึ่งอาจจะเรียกว่า Noise เพราะข้อมูลโดดไปจากเพื่อน) ทีนี้หากบังเอิญว่า ในข้อมูลที่เป็น Noise มาเจอกับ Neuron ที่มีค่าพารามิเตอร์สูงๆ จะเกิดอะไรขึ้น ผลที่เกิดขึ้นก็คือจะทำให้การคำนวณค่า loss เกิดเพี้ยนได้มาก ซึ่งเป็นสิ่งที่เราไม่ต้องการ\n",
    "\n",
    "จึงได้กล่าวว่า Overfitting คือ \"การเรียนรู้ไปจดจำ Noise มากเกินไป ทำให้พลาดการมองแนวโน้ม\"\n",
    "\n",
    "เมื่อเราทราบถึงสาเหตุ ดังนั้นการแก้ไขก็คือ พยายามทำให้ model มีพารามิเตอร์ที่ \"ไม่มีเส้นใหญ่\" มากนัก\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVW6TWT_SQ0E"
   },
   "source": [
    "### วิธีในการแก้ไข Overfitting\n",
    "\n",
    "เมื่อเราทราบสาเหตุแล้ว เราก็มาดูว่าจะมีวิธีแก้อย่างไรบ้าง\n",
    "\n",
    "1. เพิ่มข้อมูลการฝึกให้มากขึ้น เมื่อมีข้อมูลที่หลากหลายขึ้น ข้อมูลที่เป็น Noise ก็จะมีผลกับ model ลดลง การจำรูปแบบก็จะลดลง\n",
    "\n",
    "2. ทำ Cross validation เป็นการปรับปรุงการแบ่งข้อมูลใหม่ โดยแทนที่จะแบ่งข้อมูลเป็นชุดสอน (Training set) และ ชุดทดสอบ (Test set) ก็จะทำอีกแบบ คือชุดข้อมูลจะถูกแบ่งออกเป็นชุดย่อยๆ จำนวน k ชุด และ ในการทำงานจะทำงานทั้งหมด k รอบ โดยในแต่ละรอบ ชุดข้อมูล Test จะมีการสับเปลี่ยนกัน ทำให้เมื่อ train ครบทั้ง k รอบ ชุดข้อมูลทุกส่วนจะมีโอกาสเป็นทั้งชุดข้อมูลสอน และทดสอบ โดยอาจมองว่าเป็นการเพิ่มความหลากหลายของข้อมูลเพิ่มขึ้นวิธีหนึ่งนั่นเอง\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/26833433/258589390-8d815058-ece8-48b9-a94e-0e1ab53ea0f6.png\" width=\"500\"/><div>\n",
    "\n",
    "3. Data Augmentation เป็นวิธีในการเพิ่มจำนวนของชุดข้อมูลสอน (Train set) โดยการนำข้อมูลเดิมมาดัดแปลงเพื่อเพิ่มความหลากหลายของข้อมูล กรณีที่เราไม่สามารถหาข้อมูลที่มีความหลากหลายได้มากพอ วิธีการทำ Data Augmentation จะนิยมใช้กับข้อมูลที่เป็นภาพมาก เพราะสามารถเพิ่มความหลากหลายได้ง่าย เช่น เอารูปมาขยาย หรือ หมุน หรืออื่นๆ ก็จะได้ข้อมูลที่แตกต่างจากข้อมูลเดิม จากนั้นก็นำเข้าไปเป็นข้อมูลสำหรับฝึกสอน\n",
    "\n",
    "<img src=\"https://ubiai.tools/wp-content/uploads/2023/11/UKwFg.jpg\" width=\"500\"/><div>\n",
    "\n",
    "4. Early stopping เป็นวิธีการในการหยุดการ overfit โดยการหยุดการทำงานของ model เสียก่อน เช่น จากรูป เมื่อเราเห็นว่าการ overfit เริ่มเกิดขึ้น เราก็หยุดการทำงานของ model เสียก่อน\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:567/0*z19dbRlkgocQYn6t.png\" width=\"350\"/><div>\n",
    "\n",
    "\n",
    "ใน 3 วิธีแรกจะเน้นไปที่การเพิ่มข้อมูลการฝึกโดยวิธีการต่างๆ ส่วนวิธีที่ 4 คือ หยุดการฝึก แต่สำหรับวิธีที่จะกล่าวถึงต่อไป เป็นวิธีที่ใช้กระบวนการทางคณิตศาสตร์เข้ามาช่วย โดยมีเป้าหมายเพื่อที่จะ \"แกล้ง\" หรือ \"ลดความสำคัญ\" หรือ \"ลงโทษ\" (Penalty) กับพารามิเตอร์ที่มีค่ามากๆ\n",
    "\n",
    "4. L1 Regularization คำว่า Regularization แปลว่า การทำให้เป็นทั่วไป หรือ การทำให้เป็นมาตรฐาน โดยนัยคือ ต้องการให้พารามิเตอร์อยู่ในกรอบที่ต้องการ หลักการของ L1 regularization คือการเพิ่มเทอมลงโทษเข้าไปใน loss function ของโมเดล โดยเทอมลงโทษนี้เป็นค่าสัมบูรณ์ของค่าสัมประสิทธิ์ ดังนี้:\n",
    "\n",
    "\n",
    ">  $  \\text{Loss} = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} |w_j| $\n",
    "<br>\n",
    "\n",
    "จะเห็นว่าสมการแบ่งออกเป็น 2 เทอม โดยเทอมแรก ก็คือ Loss Function นั่นเอง ดังนั้น L1 regularization ก็คือการเพิ่มเทอมที่ 2 เข้าไป โดยเอาค่า w ทั้งหมดมาทำ absolute แล้วบวกกัน จากนั้นคูณด้วย $\\lambda$ ซึ่งเป็น coefficient ซึ่งใช้ในการควบคุมความแรงของการลงโทษ (penalty) หาก $\\lambda$ มีค่าสูง การลงโทษจะมีความแรงมาก ทำให้ค่าสัมประสิทธิ์ $w_j$ ถูกปรับให้มีค่าน้อยลงมากและบางตัวอาจเป็น 0 ได้\n",
    "\n",
    "5. L2 Regularization จะคล้ายกับ L1 แต่เปลี่ยนเป็นยกกำลังสอง ตามสมการ ความแตกต่างจาก L1 คือ L2 จะไม่ทำให้พารามิเตอร์เป็น 0 แต่จะเป็นการเกลี่ยพารามิเตอร์ให้ใกล้กันมากขึ้น\n",
    "\n",
    "> $ \\text{Loss} = \\frac{1}{2m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{n} w_j^2 $\n",
    "\n",
    "ุ6. Dropout Regularization วิธีการนี้จะเป็นการเกลี่ยพารามิเตอร์อีกรูปแบบหนึ่ง โดยจะเป็นการสุ่มตัด Neuron บางตัวออกไปในระหว่างการเรียนรู้ของโมเดล อาจมองได้ว่าเป็นการแกล้ง model ก็ได้ โดยในแต่ละรอบการทำงาน Neuron บางตัวจะถูกกำหนดให้มีน้ำหนักเป็น 0 โดยการสุ่ม ซึ่งนิวรอลที่มีน้ำหนักเป็น 0 จะไม่ส่งผลต่อการทำงาน การทำซ้ำเช่นนี้ไปเรื่อยๆ ในกระบวนการสอน จะทำให้นิวรอลเน็ตเวิร์กพยายามไม่ขึ้นอยู่กับ (Dependent) Neuron ใด Neuron หนึ่งมากเกินไป ทำให้ส่งผลต่อการลดโอกาสการเกิด Overfitting ได้นั่นเอง วิธีการนี้เราอาจมองว่าคล้ายกับการเรียนรู้ของมนุษย์ เพราะมนุษย์จะมีการลืมบางข้อมูล ดังนั้นในรอบการเรียนรู้ก็จะทำให้โมเดลลืมบางพารามิเตอร์ไปบ้าง จะทำให้ model เรียนรู้แนวโน้ม (trend) มากขึ้น\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/309206911/figure/fig3/AS:418379505651712@1476760855735/Dropout-neural-network-model-a-is-a-standard-neural-network-b-is-the-same-network.png\" width=\"600\"/><div>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBIB4mNDS7OU"
   },
   "source": [
    "### ปรับปรุง model โดยการเพิ่ม node และ Layer\n",
    "\n",
    "เอาละ! คราวนี้เราจะกลับมาปรับปรุง Model โดยเพิ่มจำนวน Node และ Layer เข้าไป โดยหวังว่าจะเรียนรู้ได้มากขึ้น โดยได้เพิ่มเป็น 2 Layer โดยกำหนดให้ Layer แรกมี 160 node และ Layer ที่ 2 จำนวน 80 node\n",
    "\n",
    "โดยเขียนเป็น คลาสชื่อ ThaiNumberNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpzUew4igVhc"
   },
   "outputs": [],
   "source": [
    "class ThaiNumberNN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEmn7R5ebUdV"
   },
   "source": [
    "### ทดสอบการทำงาน\n",
    "\n",
    "เมื่อปรับโมเดลเรียบร้อย ก็ทดลอง train 100 epoch เหมือนเดิม แล้วดูว่าจะได้ผลดีขึ้นไหม\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 510197,
     "status": "ok",
     "timestamp": 1720269937381,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "ah6x3wc8hkvp",
    "outputId": "7ff20c06-70cd-4a67-cf8c-0d0195f818c4"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "\n",
    "TN_model2 = ThaiNumberNN2().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(TN_model2.parameters(), lr=0.01)\n",
    "\n",
    "TN_model2, TN_model2_history = train(\n",
    "    TN_model2, n_epochs, loss_fn, optimizer, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1720269952665,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "dAbsrsiBh5y6",
    "outputId": "54740b43-1819-432d-d216-70558f3bf369"
   },
   "outputs": [],
   "source": [
    "plot_graph(TN_model2_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-0ja4E07bYY"
   },
   "source": [
    "### วิเคราะห์กราฟ\n",
    "\n",
    "เมื่อนำข้อมูลของ Accuracy มา plot เป็นกราฟก็จะเห็นว่าความถูกต้องของ traning data เพิ่มเป็นประมาณ 98 % เรียกได้ว่า Model เรียนรู้ได้ดีขึ้น และ Accuracy ของ validation data ก็เพิ่มขึ้นเป็น 72 % ก็ถือว่าดีขึ้น\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f46a1hvAjdPL"
   },
   "source": [
    "### เพิ่ม Dropout\n",
    "\n",
    "คราวนี้เราจะลองลด Overfitting โดยการเพิ่ม Dropout ดูว่าจะได้ผลอย่างไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJy60mkSOPLg"
   },
   "outputs": [],
   "source": [
    "class DropoutThaiNumberNN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 160)\n",
    "        self.fc2 = nn.Linear(160, 80)\n",
    "        self.fc3 = nn.Linear(80, 10)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 505975,
     "status": "ok",
     "timestamp": 1720270491781,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "8aQH951fOSgq",
    "outputId": "389b420e-59b7-405b-a323-ad525f735165"
   },
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "drop_model = DropoutThaiNumberNN3().to(device)  # Initialize a model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(drop_model.parameters(), lr=0.01)\n",
    "\n",
    "dropout_model, dropout_model_history = train(drop_model, n_epochs, loss_fn, optimizer, train_loader, validation_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1720270602977,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "NE8ZOJ8YObIC",
    "outputId": "8a300d80-afda-4fc3-f108-cc25612ac73f"
   },
   "outputs": [],
   "source": [
    "plot_graph(dropout_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTjC75hImIGG"
   },
   "source": [
    "จะเห็นว่าความถูกต้องสูงขึ้น และ overfitting ลดลง เล็กน้อยเท่านั้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkcZOLr38wlK"
   },
   "source": [
    "### Image Augmentation\n",
    "\n",
    "เราจะปรับปรุงต่อโดยใช้เทคนิคเพิ่มความหลากหลายโดยใช้ Image Augmentation โดยใช้คำสั่ง RandomAffine ใน Pytorch โดยคำสั่งนี้จะสั่งให้ random โดยทำ operation ของ TorchVision มาใช้ ได้แก่ degrees คือ การหมุนภาพระหว่าง -10 ถึง 10 องศา translate คือ การเลื่อนซ้ายขวาบนล่าง ก็ใช้ประมาณ 10 % แต่ scale คือ การขยายภาพ จะไม่ใช้ จึงให้ค่าเป็นเท่าเดิม\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJfkgfCTOemk"
   },
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    # add RandomAffine\n",
    "    transforms.RandomAffine(degrees=(-10, 10), translate=(0.0, 0.1), scale=(1, 1)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((28, 28)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRj1EzRnnaBb"
   },
   "source": [
    "คราวนี้มาลองดูภาพที่สร้างขึ้นจาก transform โดยมีการทำ Data Augmentation จะเห็นได้ว่าเปลี่ยนไปจากเดิม"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1076,
     "status": "ok",
     "timestamp": 1720270736416,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "xPJGZfTtOiJ-",
    "outputId": "aafd28c3-416c-477e-efa7-5861c274b01f"
   },
   "outputs": [],
   "source": [
    "path = glob(\"curriculum/Dataset/thai-handwritten-dataset/*/*\")[30]\n",
    "img = Image.open(path)\n",
    "\n",
    "transformed_img = train_transform(img).squeeze(0)\n",
    "\n",
    "plt.imshow(transformed_img, cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoNEaLEbnrAe"
   },
   "source": [
    "พอเราสร้างชุดข้อมูลใหม่ ก็จะต้องมาสร้าง dataset ชุดใหม่ด้วย"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rW_hX8W1OizD"
   },
   "outputs": [],
   "source": [
    "train_thaidigit_dataset = ThaiHandNumData(\"data/train/\", transform=train_transform)\n",
    "validation_thaidigit_dataset = ThaiHandNumData(\"data/test/\", transform=val_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXw6t5EBOlFE"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_thaidigit_dataset, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_thaidigit_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2Chbo_bn2Gz"
   },
   "source": [
    "คราวนี้ก็ลงมือ train เนื่องจากเราทำ Data Augmentation ก็เปรียบเสมือนกับมีข้อมูลเพิ่มขึ้น ดังนั้นจะเพิ่ม epoch เป็น 150 รอบ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 805153,
     "status": "ok",
     "timestamp": 1720271560980,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "VnIY1g7IOnB3",
    "outputId": "463a2fb4-7e82-4772-9818-4ed9fe9169fc"
   },
   "outputs": [],
   "source": [
    "# Let's train the model with augmented data\n",
    "n_epochs = 150\n",
    "augmented_model = DropoutThaiNumberNN3().to(device)  # Initialize the model\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(augmented_model.parameters(), lr=0.01)\n",
    "\n",
    "augmented_model, augmented_model_history = train(\n",
    "    augmented_model, n_epochs, loss_fn, optimizer, train_dataloader, validation_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q32fXrBzogc8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create evaluation function for the model\n",
    "def evaluate(val_dir, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    paths = glob(os.path.join(val_dir, \"*\", \"*\"))\n",
    "    predictions = []\n",
    "    for idx, path in enumerate(paths):\n",
    "        img = 1 - transform(Image.open(path)).to(device)\n",
    "        pred = model(img)\n",
    "        predictions.append({\n",
    "            \"path\": path,\n",
    "            \"prediction\": int(pred.argmax(dim=1).item()),\n",
    "            \"label\": int(Path(path).parent.name)\n",
    "        })\n",
    "\n",
    "    results = pd.DataFrame(predictions)\n",
    "    # Calculate accuracy\n",
    "    accuracy = (results.prediction == results.label).sum() / len(results)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "executionInfo": {
     "elapsed": 2809,
     "status": "ok",
     "timestamp": 1720271760143,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "ptO4Rfp7Ou8p",
    "outputId": "fbed2ea3-d175-4eef-d97a-6d274ae34027"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pred_df = evaluate(\"data/test/\", augmented_model, device)\n",
    "pred_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 1550,
     "status": "ok",
     "timestamp": 1720271781943,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "4RVckpvMOxBc",
    "outputId": "f81d4c44-6383-4781-9109-c9702de9b71b"
   },
   "outputs": [],
   "source": [
    "plot_graph(augmented_model_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIdUUNHFtUor"
   },
   "source": [
    "### วิเคราะห์กราฟ\n",
    "\n",
    "จากกราฟจะเห็นได้ว่า overfitting ลดลงอย่างชัดเจนแม้ Accuracy จะได้เพียง 73.71 ซึ่งถือว่ายังไม่น่าพอใจ แต่เรายังใช้เทคนิคต่างๆ ในการปรับปรุงได้อีก โดยหากปรับปรุงให้ดีแล้ว น่าจะได้กราฟประมาณนี้\n",
    "\n",
    "<div>\n",
    "<br>\n",
    "<img src=\"https://github.com/khthana/AI_resource/blob/main/thainum_final.JPG?raw=true\" width=\"900\"/><div>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9DkwVytue0e"
   },
   "source": [
    "### การประเมิน model โดยใช้ confusion matrix\n",
    "\n",
    "ในการประเมิน model จะมีอยู่ด้วยกันหลายวิธีการ วิธีการหนึ่งเรียกว่า confusion matrix ซึ่งมักจะใช้กับงานที่เป็น Classification วิธีการ คือ นำผลการทำนาย กับ label หรือข้อมูลจริงทุกข้อมูล มา plot เป็น matrix ซึ่งข้อมูลที่ทำนายได้ถูกจะอยู่ตรงเส้นทะแยงมุม และจะทำให้เห็นว่าข้อมูลที่ผิด มักจะผิดที่ตัวใดบ้าง ซึ่งอาจจะนำข้อมูลนี้มาปรับปรุงโมเดลต่อไป\n",
    "\n",
    "นอกเหนือจากการใช้ confusion matrix ก็จะมีการประเมินต่างๆ ที่ควรจะรู้จักดังนี้\n",
    "- Precision (ความแม่นยำ): เปอร์เซ็นต์ของตัวอย่างที่ทำนายว่าเป็นบวกที่เป็นบวกจริง คำนวณจาก True Positives / (True Positives+False Positives)\n",
    "- Recall (ความครอบคลุม): เปอร์เซ็นต์ของตัวอย่างบวกที่ถูกทำนายถูกต้อง คำนวณจาก True Positives / (True Positives+False Negatives)\n",
    "- F1-Score: ค่าคะแนนที่เป็น Harmonic Mean ระหว่าง Precision และ Recall คำนวณจาก 2 x (Precision×Recall​)/(Precision+Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 1367,
     "status": "ok",
     "timestamp": 1720271805283,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "LbFXA03DOpst",
    "outputId": "cf1e00a3-521b-4870-e066-d86d34380b97"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(pred_df.label, pred_df.prediction, labels=range(10))\n",
    "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=range(10))\n",
    "display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbq9OPZKvbhm"
   },
   "source": [
    "## การถอด parameter ออกมาแสดง\n",
    "\n",
    "ในกรณีที่เราอยากรู้ว่าในแต่ละ neuron มีพารามิเตอร์อะไรบ้าง นอกเหนือจากการใช้คำสั่ง state_dict() แล้ว เรายังสามารถจะนำ model parameter ของ layer ที่ 1 (fc1) ของโมเดล DropoutThaiNumberNN3 มา plot เป็น image ขนาด 28x28 ได้ด้วย เนื่องจาก weight มีขนาด 28x28 อยู่แล้ว เพียงแต่ถูกยืดอยู่ เราก็เพียงแต่นำมาจัดเรียงเป็น 28x28 ก็สามารถนำมาแสดงเป็นภาพได้ ทำให้เราสามารถเห็นข้อมูลภายใน neuron ซี่งจะเรียกข้อมูลนี้ว่า feature\n",
    "\n",
    "โดยเราจะต้องทำการ extract weights จาก layer fc1 แล้ว reshape weights นั้นให้เป็นขนาด 28x28 จากนั้นใช้ library เช่น matplotlib ในการ plot ภาพนี้\n",
    "\n",
    "ในกรณีของ layer fc1 ในโมเดลนี้ เป็น fully connected layer (หรือ linear layer) ที่มี input size 28*28 = 784 ดังนั้น weights ของ fc1 จะมีขนาด (160, 784) ซึ่งหมายความว่าแต่ละ neuron ใน fc1 จะมี weight vector ที่ยาว 784 ดังนั้น เราสามารถเลือก neuron ใด neuron หนึ่งจาก fc1 แล้ว reshape weight vector ของมันให้เป็นขนาด 28x28 เพื่อ plot เป็นภาพ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 2557,
     "status": "ok",
     "timestamp": 1720274071252,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "LJZANZz4x8A0",
    "outputId": "54eedaa4-1ace-4958-c19e-2a123caf96a0"
   },
   "outputs": [],
   "source": [
    "# Extract the weights of the first layer (fc1)\n",
    "fc1_weights = augmented_model.fc1.weight.data\n",
    "\n",
    "# Select the weights of the first neuron (or any neuron you want to visualize)\n",
    "neuron_idx = 1  # Index of the neuron to visualize\n",
    "weight_vector = fc1_weights[neuron_idx]\n",
    "\n",
    "# Reshape the weight vector to 28x28\n",
    "weight_image = weight_vector.view(28, 28).cpu().numpy()\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(weight_image, cmap='gray')\n",
    "plt.title(f'Weights of neuron {neuron_idx} in fc1')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "executionInfo": {
     "elapsed": 2988,
     "status": "ok",
     "timestamp": 1720274376279,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "S7XLJG2wy0WQ",
    "outputId": "01eb3cc4-6579-449f-ac54-187d4f3ed0cd"
   },
   "outputs": [],
   "source": [
    "# Extract the weights of the first layer (fc1)\n",
    "fc1_weights = augmented_model.fc1.weight.data\n",
    "\n",
    "# Number of neurons in fc1\n",
    "num_neurons = fc1_weights.shape[0]\n",
    "\n",
    "# Prepare a grid to store all images\n",
    "grid_height = 10\n",
    "grid_width = 16\n",
    "image_height = 28\n",
    "image_width = 28\n",
    "\n",
    "# Create an empty array to hold the grid image\n",
    "grid_image = np.zeros((grid_height * image_height, grid_width * image_width))\n",
    "\n",
    "# Fill the grid with the weight images\n",
    "for i in range(num_neurons):\n",
    "    weight_vector = fc1_weights[i]\n",
    "    weight_image = weight_vector.view(image_height, image_width).cpu().numpy()\n",
    "\n",
    "    row = i // grid_width\n",
    "    col = i % grid_width\n",
    "\n",
    "    grid_image[row * image_height: (row + 1) * image_height,\n",
    "               col * image_width: (col + 1) * image_width] = weight_image\n",
    "\n",
    "# Plot the grid image\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(grid_image, cmap='gray')\n",
    "plt.title('Weights of all neurons in fc1')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZ5zZpUDNkch"
   },
   "source": [
    "รูปด้านบนอาจจะดูไม่ค่อยรู้เรื่อง เพราะข้อมูลเป็นตัวเลข และ การทำงานไม่ใช่ Convolution แต่ในบางงานที่เป็นรูปภาพ รูปของ feature map สามารถบอกสิ่งที่เกิดขึ้นในการทำงานของ Neural Network ได้\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4800/format:webp/1*LlRAQHT0ktl_33VUnDhoIg.png\">\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:4426/format:webp/1*_uk1cKXslKo1gDHOWzljXA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8vGnBzcIDP6"
   },
   "source": [
    "# Save Model\n",
    "\n",
    "เอาละ! มาถึงตรงนี้ เราก็มี model ไว้สำหรับใช้งานแล้ว เนื่องจากการ train model ค่อนข้างจะเสียเวลามาก ดังนั้นหากต้องทำทุกครั้งน่าจะไม่ดี จึงควรจะ save model เอาไว้เพื่อนำมาใช้งานภายหลัง\n",
    "\n",
    "ในการ save model ก็คือ การ save state_dict นั่นเอง"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 450,
     "status": "ok",
     "timestamp": 1720274409563,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "4MZNFSUDWBZd",
    "outputId": "3cfc7ab7-2e38-45b5-aa7e-1afb2070d1d1"
   },
   "outputs": [],
   "source": [
    "augmented_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1720274456043,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "kZ5MV0K7WKcq",
    "outputId": "5bd00f77-0c37-42a8-c6a3-b18eae611242"
   },
   "outputs": [],
   "source": [
    "augmented_model.state_dict()[\"fc1.weight\"].shape, augmented_model.state_dict()[\"fc1.bias\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bo36qSCgWRiz"
   },
   "outputs": [],
   "source": [
    "save_path = \"thai_digit.pth\"\n",
    "torch.save(augmented_model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvF5Lm3JWeEN"
   },
   "source": [
    "จากนั้นก็ต้อง download มาเก็บไว้ มิฉะนั้น google colab จะลบข้อมูลออกหมด"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "otXwinA-W6eB"
   },
   "source": [
    "# Load Model\n",
    "\n",
    "เมื่อต้องการนำ Model ที่ save เก็บไว้มาใช้อีกครั้ง ให้ทำขั้นตอนเหมือนกับที่ผ่านมา ยกเว้นไม่ต้อง train แต่โหลดข้อมูลจากไฟล์เข้าสู่ model ได้เลย\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1720274838584,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "ilpwq7CEWdBM",
    "outputId": "f2666af7-9c07-41b3-f1ce-a2ae7ae8a950"
   },
   "outputs": [],
   "source": [
    "model = DropoutThaiNumberNN3()\n",
    "model.load_state_dict(torch.load(\"thai_digit.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFQvxRgKX8Ew"
   },
   "source": [
    "# สรุป\n",
    "\n",
    "ในบทเรียนนี้เราได้เรียนการนำ Pytorch มาใช้งาน โดยเน้นที่ Classification และกล่าวถึงการทำงานของ Logistic Regression, Softmax และ Cross Entropy Loss ซึ่งใช้ในกระบวนการ Classification นอกจากนั้นยังได้กล่างถึงเรื่องของ Optimization การเกิด Overfitting และแนวทางการแก้ไข ตลอดจนการประเมินผลโมเดล การ save และ load โมเดล ซึ่งเป็นพื้นฐานของ deep learning ต่อไป\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IewR4-EdNNT"
   },
   "source": [
    "# Optional\n",
    "\n",
    "หากนักเรียนคนใดต้องการ Dataset สำหรับทดลองสร้าง model เพิ่มเติม จะมี Dataset ต่อไปนี้"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS2IdMcJdeD-"
   },
   "source": [
    "## Fasion MNIST dataset\n",
    "\n",
    "ชุดข้อมูล Fashion-MNIST คือชุดรูปภาพสินค้าแฟชั่น จากเว็บไซต์ Zalando ประกอบด้วยรูปภาพสำหรับฝึกฝนโมเดล 60,000 รูป และรูปภาพสำหรับทดสอบอีก 10,000 รูป รูปภาพแต่ละรูปมีขนาด 28x28 พิกเซล เป็นภาพขาวดำ และมีการติดป้ายกำกับประเภทของเสื้อผ้า 1 ใน 10 ประเภท\n",
    "\n",
    "รูปภาพแต่ละรูปมีความสูง 28 พิกเซล กว้าง 28 พิกเซล ดังนั้นจะมีพิกเซลทั้งหมด 784 พิกเซล แต่ละพิกเซลจะมีค่าตั้งแต่ 0 ถึง 255 โดยค่าที่สูงกว่าหมายถึงความมืดมากกว่า ค่าเหล่านี้แทนระดับความสว่างของจุดนั้น ๆ ชุดข้อมูลสำหรับฝึกฝนและทดสอบจะมี 785 คอลัมน์ คอลัมน์แรกจะเป็นหมายเลขประเภทของเสื้อผ้า (ตามที่ระบุไว้ด้านบน) คอลัมน์ที่เหลือจะเป็นค่าของแต่ละพิกเซลในภาพนั้น\n",
    "\n",
    "ข้อมูลแต่ละตัวอย่างสำหรับฝึกฝนและทดสอบจะมีการกำหนดประเภทไว้ดังนี้:\n",
    "\n",
    "0 T-shirt/top 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Sandal 6 Shirt 7 Sneaker 8 Bag 9 Ankle boot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CG2aFGeod-Jd"
   },
   "source": [
    "### Import และ โหลดข้อมูล"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeo7Zb9ud7XH"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data loading and preprocessing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 1254,
     "status": "ok",
     "timestamp": 1720767055225,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "rGqE-uiwqrYm",
    "outputId": "919a6ebe-ee45-4b84-f7d7-048c0cb887d0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "images, labels = next(iter(train_loader))\n",
    "image = images[10]\n",
    "label = labels[10]\n",
    "image = image.permute(1, 2, 0)\n",
    "image = image * 0.5 + 0.5\n",
    "plt.imshow(image)\n",
    "plt.title(f'Label: {label}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiUENYszeV0g"
   },
   "source": [
    "## Horses or Humans Dataset\n",
    "\n",
    "ชุดข้อมูลนี้ประกอบด้วยภาพเรนเดอร์ 500 ภาพของม้าหลายสายพันธุ์ในท่าทางต่างๆ และสถานที่ต่างๆ นอกจากนี้ยังมีภาพเรนเดอร์ของมนุษย์ 527 ภาพในท่าทางและพื้นหลังที่แตกต่างกัน มีความหลากหลายของมนุษย์ ดังนั้นจึงมีทั้งผู้ชายและผู้หญิง รวมถึงชาวเอเชีย แอฟริกันใต้ เอเชียใต้ และผิวขาวอยู่ในชุดข้อมูลฝึกอบรม ชุดข้อมูลการตรวจสอบเพิ่มตัวเลขอีกหกตัวเพื่อให้แน่ใจว่าข้อมูลมีความกว้าง\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19970,
     "status": "ok",
     "timestamp": 1720856376248,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "boPny4W6tzB7",
    "outputId": "2fa0ec7a-fd4c-414f-8562-70f008186ffa"
   },
   "outputs": [],
   "source": [
    "# Step 1: Install and import necessary libraries\n",
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Step 2: Download the dataset\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/learning-datasets/horse-or-human.zip \\\n",
    "    -O /tmp/horse-or-human.zip\n",
    "\n",
    "!wget --no-check-certificate \\\n",
    "    https://storage.googleapis.com/learning-datasets/validation-horse-or-human.zip \\\n",
    "    -O /tmp/validation-horse-or-human.zip\n",
    "\n",
    "# Step 3: Extract the datasets\n",
    "local_zip = '/tmp/horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp/horse-or-human')\n",
    "zip_ref.close()\n",
    "\n",
    "local_zip = '/tmp/validation-horse-or-human.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('/tmp/validation-horse-or-human')\n",
    "zip_ref.close()\n",
    "\n",
    "# Step 4: Prepare the data\n",
    "data_dir_train = '/tmp/horse-or-human'\n",
    "data_dir_val = '/tmp/validation-horse-or-human'\n",
    "\n",
    "# Define transforms for training and validation data\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((300, 300)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load the datasets with ImageFolder\n",
    "train_data = datasets.ImageFolder(data_dir_train, transform=train_transforms)\n",
    "val_data = datasets.ImageFolder(data_dir_val, transform=val_transforms)\n",
    "\n",
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=20, shuffle=False)\n",
    "\n",
    "# Print the class names and number of samples\n",
    "print(f'Train dataset classes: {train_data.classes}')\n",
    "print(f'Number of training samples: {len(train_data)}')\n",
    "print(f'Number of validation samples: {len(val_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "executionInfo": {
     "elapsed": 1700,
     "status": "ok",
     "timestamp": 1720856387937,
     "user": {
      "displayName": "Thana Hongsuwan",
      "userId": "11586563108834032651"
     },
     "user_tz": -420
    },
    "id": "40_uFIoKt0gX",
    "outputId": "316cb9f4-18e6-45cd-cf79-0fb3276e5247"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_data[10]\n",
    "image = image.permute(1, 2, 0)\n",
    "image = image * 0.5 + 0.5\n",
    "plt.imshow(image)\n",
    "plt.title(f'Label: {train_data.classes[label]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndW6aY-puAJE"
   },
   "source": [
    "# Exercise 1\n",
    "\n",
    "จากชุดข้อมูลลายมือเขียนภาษาไทย 0-9 ให้ปรับแต่ง Model โดยใช้เทคนิคต่างๆ (เฉพาะที่เรียนมา) เช่น ขนาด pixel, batch size, ชั้นและจำนวนของ Neuron, Optimization Algorithm, Loss Function, Activation Function, Learning Rate, Data Augmentaion, Dropout, Regularization, Cross Validation\n",
    "\n",
    "โดยมีเป้าหมายให้ Training Accuracy และ Validation Accuracy มากที่สุด โดยมี Overfit น้อยที่สุด\n",
    "\n",
    "ให้ capture กราฟและผลการทำงาน (epoch print) พร้อมทั้งอธิบายการปรับ Hyperparameter ที่เลือกใช้พร้อมเหตุผล ส่งเป็นไฟล์ PDF\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DANeR7lXtQFd"
   },
   "source": [
    "## Exercise 2\n",
    "\n",
    "ให้ใช้ link ต่อไปนี้ เพื่อทำชิ้นงาน\n",
    "\n",
    "https://github.com/NextGen-AI-Camp/curriculum/tree/main/Week%234/NN%233/NextGen_AI_Camp_Neural_Network3_Exercise2.ipynb\n",
    "\n",
    "ใน Notebook จะสร้างข้อมูลในรูปแบบ Spiral โดยข้อมูลจะมีจำนวน 300 ตำแหน่ง แบ่งออกเป็น 3 Class ข้อมูลจะอยู่ใน X และ Label จะอยู่ใน y จากนั้นจะนำไปสร้างเป็น Dataset และแบ่งเป็น train_dataset, val_dataset และ นำไปสร้างเป็น DataLoader อีกต่อหนึ่งเป็น train_loader และ val_loader\n",
    "\n",
    "ใน Notebook จะมี function plot_decision_boundary ซึ่งต้องการ Parameter 3 ตัว คือ model, X และ y เพื่อแสดงขอบเขตของแต่ละคลาส\n",
    "ให้ทำตาม TODO ให้ครบ\n",
    "ให้ส่งเป็นไฟล์ .ipynb โดยตั้งชื่อไฟล์ขึ้นต้นเป็นเลขของค่าย 3 หลัก ภายในไฟล์ให้ใส่ E-Mail ด้วย เผื่อมีความผิดพลาด\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMkKeRVKJh08N4rndC+mmax",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0ad3c5748e81445d9e2407e12142c31d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15a776dcd0dd407a8d8125cfd82be9ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24fcbb8a977146f58a9a27a3833c296a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d2ccb6b6473947f3be0eef2c91fab70b",
      "placeholder": "​",
      "style": "IPY_MODEL_fe8e645fce5f4df3833a0a45e73fded4",
      "value": "100%"
     }
    },
    "289c67385f884040982babd98f6f9dfe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "29c3ed3fad894095ae2b773053177c78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "361de2a68f8b478fb85c631ba11e041b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3ff6a10b199a46179025cb6c9248c10b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c92ec73b56b4b7e8d2926557822a34a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8e99babf7f324a7b97e360134b6cf92a",
       "IPY_MODEL_823824e590bd49c0bfef8ee84e85f22d",
       "IPY_MODEL_8baeaec3cc93441597e76d19528a723b"
      ],
      "layout": "IPY_MODEL_3ff6a10b199a46179025cb6c9248c10b"
     }
    },
    "4e16a362c9d441e79f483d3a73215253": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5be7368f19b9438baddd2f18ad1c5eae": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5c6736ca59e1486f902a5ae8b7bb0006": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d432e78655248fcae6dc0b8dbf83f19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "823824e590bd49c0bfef8ee84e85f22d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f48426d7d2244d82b64ef94b271c9384",
      "max": 175,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_361de2a68f8b478fb85c631ba11e041b",
      "value": 175
     }
    },
    "8baeaec3cc93441597e76d19528a723b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_29c3ed3fad894095ae2b773053177c78",
      "placeholder": "​",
      "style": "IPY_MODEL_289c67385f884040982babd98f6f9dfe",
      "value": " 175/175 [00:00&lt;00:00, 2416.95it/s]"
     }
    },
    "8e99babf7f324a7b97e360134b6cf92a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5c6736ca59e1486f902a5ae8b7bb0006",
      "placeholder": "​",
      "style": "IPY_MODEL_4e16a362c9d441e79f483d3a73215253",
      "value": "100%"
     }
    },
    "bf43f2e8af014cf4a67c27ffb39a7e8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15a776dcd0dd407a8d8125cfd82be9ee",
      "placeholder": "​",
      "style": "IPY_MODEL_5d432e78655248fcae6dc0b8dbf83f19",
      "value": " 1575/1575 [00:00&lt;00:00, 4039.28it/s]"
     }
    },
    "c438f1179a154b5eb752f9986e4d127a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e43cb91532fe4153b7565ef2e35902b9",
      "max": 1575,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5be7368f19b9438baddd2f18ad1c5eae",
      "value": 1575
     }
    },
    "d2ccb6b6473947f3be0eef2c91fab70b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e07c2e785e43400d80c10691cf8db382": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_24fcbb8a977146f58a9a27a3833c296a",
       "IPY_MODEL_c438f1179a154b5eb752f9986e4d127a",
       "IPY_MODEL_bf43f2e8af014cf4a67c27ffb39a7e8b"
      ],
      "layout": "IPY_MODEL_0ad3c5748e81445d9e2407e12142c31d"
     }
    },
    "e43cb91532fe4153b7565ef2e35902b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f48426d7d2244d82b64ef94b271c9384": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fe8e645fce5f4df3833a0a45e73fded4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
